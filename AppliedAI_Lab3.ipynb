{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "479e0f2f-ef53-4030-9574-da7b33801aac",
   "metadata": {},
   "source": [
    "# \tThe\tlanguage classification\tproblem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468b3824-7b0c-48e1-9f50-a2b9b95cb23a",
   "metadata": {},
   "source": [
    "Paper experiment setup: While in this study, in order to enable training of SOMs, each language corpus was divided into samples where the length of each sample was set to 1, 000 symbols. The data for each language was preprocessed such that the text included only lower case letters and spaces. All punctuation was removed. Lastly, all text used the 26-letter ISO basic Latin alphabet, i.e., the alphabet for both training and test data was the same and it included 27 symbols. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2868945-ffa8-45c1-8dd8-064252d8aae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baaed5ca-80e9-44d0-8c24-2ef515231c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = 'abcdefghijklmnopqrstuvwxyz '  # 27 symbols including space\n",
    "alphabet_size = len(alphabet)\n",
    "hd_dimension1 = 100  \n",
    "hd_dimension2 = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea39213c-2a20-4e96-8261-80cc804def8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(text, n=3):\n",
    "    ngrams = []\n",
    "    for i in range(len(text) - n + 1):\n",
    "        ngram = text[i:i + n]\n",
    "        ngrams.append(ngram)\n",
    "    \n",
    "    return ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "109ff75a-c984-4553-9b95-e23ff31e1906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "4\tAh, dernier truc : l’idéal est de vider la cuisine d’un trait en prenant bien le temps de ranger et non pas petit à petit mais dans l’urgence comme j’ai fait car cela devient vite le bazar.\n",
      "\n",
      "Preprocessed text:\n",
      "ah dernier truc  lidal est de vider la cuisine dun trait en prenant bien le temps de ranger et non pas petit  petit mais dans lurgence comme jai fait car cela devient vite le bazar\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z ]', '', text)\n",
    "    return text\n",
    "\n",
    "example1 = '4\tAh, dernier truc : l’idéal est de vider la cuisine d’un trait en prenant bien le temps de ranger et non pas petit à petit mais dans l’urgence comme j’ai fait car cela devient vite le bazar.'\n",
    "processed_text1 = preprocess(example1)\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(example1)\n",
    "print(\"\\nPreprocessed text:\")\n",
    "print(processed_text1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6108ef59-c713-4e64-bbb0-bc328b7bbc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_example1 = ngrams(processed_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d38256b-8539-4873-8ebe-bbda04991fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(file_path, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    with open(file_path, 'r') as file:\n",
    "        sentences = file.readlines()\n",
    "\n",
    "    random.shuffle(sentences)\n",
    "\n",
    "    total_sentences = len(sentences)\n",
    "    train_end = int(train_ratio * total_sentences)\n",
    "    val_end = train_end + int(val_ratio * total_sentences)\n",
    "\n",
    "    train_sentences = sentences[:train_end]\n",
    "    val_sentences = sentences[train_end:val_end]\n",
    "    test_sentences = sentences[val_end:]\n",
    "\n",
    "    base, ext = os.path.splitext(file_path)\n",
    "    with open(f\"{base}_train{ext}\", 'w') as train_file:\n",
    "        train_file.writelines(train_sentences)\n",
    "    with open(f\"{base}_val{ext}\", 'w') as val_file:\n",
    "        val_file.writelines(val_sentences)\n",
    "    with open(f\"{base}_test{ext}\", 'w') as test_file:\n",
    "        test_file.writelines(test_sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a628cc-2e69-43a5-9c9a-31a5e6ff80eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_stat(ngrams):\n",
    "    s_i = np.zeros(alphabet_size ** n)\n",
    "\n",
    "    # Create a mapping from n-gram to index in vector s\n",
    "    ngram_to_index = defaultdict(lambda: len(ngram_to_index))\n",
    "    \n",
    "    for ngram in ngrams:\n",
    "        index = ngram_to_index[ngram]\n",
    "        s_i[index] += 1\n",
    "\n",
    "    return s_i  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5de0be-c15e-45c2-bfb4-16c743df7e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_stat(folder_path):\n",
    "    dataset = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            sentences = read_sentences(file_path)\n",
    "            for sentence in sentences:\n",
    "                preprocessed_sentence = preprocess(sentence)\n",
    "                ngrams_sentence = ngrams(preprocessed_sentence)\n",
    "                s_i = ngram_sta(ngrams_sentence)\n",
    "            dataset[filename] = s_i  \n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fbb114-1d7e-48c9-8888-67e6a98b2de7",
   "metadata": {},
   "source": [
    "# Hyperdimensional Centroid Language classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d97709e-0e20-426b-9639-1e3d241ddc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_hd_vectors(ngrams, d):\n",
    "    hd_vectors = {}\n",
    "    for ngram in ngrams:\n",
    "        hd_vectors[ngram] = np.random.choice([-1, 1], size=(d,))\n",
    "    return hd_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f140aae-32f9-4a0c-8018-a08b6b7babd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_memory(alphabet_size, hd_dimension):\n",
    "    H = np.random.choice([-1, 1], size=(hd_dimension, alphabet_size))\n",
    "    return H\n",
    "\n",
    "def get_hd_vector(symbol_index, item_memory):\n",
    "    # Retrieve the HD vector for the given symbol index from item memory H\n",
    "    return item_memory[:, symbol_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f21a82f-eaf5-4263-9d5b-8b447ad59c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item memory (H):\n",
      "(100, 27)\n",
      "\n",
      "HD vector for symbol 'a' (H[a]):\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "# Initialize item memory\n",
    "item_memory = init_memory(alphabet_size, hd_dimension1)\n",
    "\n",
    "# Retrieve HD vector for a specific symbol (e.g., 'a')\n",
    "symbol_index = alphabet.index('a')\n",
    "hd_vector_a = get_hd_vector(symbol_index, item_memory)\n",
    "\n",
    "print(f\"Item memory (H):\\n{item_memory.shape}\")\n",
    "print(f\"\\nHD vector for symbol 'a' (H[a]):\\n{hd_vector_a.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5319458-14a0-4331-8902-44bc42af87b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rho(hd_vector, times, shift=5):\n",
    "    if times == 0:\n",
    "        return hd_vector\n",
    "    else:\n",
    "        # Rotate the hd_vector by 'shift' positions\n",
    "        return rho(np.roll(hd_vector, shift), shift, times - 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba79141-a8b1-4226-9399-c4a0b76abbfc",
   "metadata": {},
   "source": [
    "# forming HD vector of an n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e0396d-4309-445b-8a8c-251bef0c38b6",
   "metadata": {},
   "source": [
    "From paper: for the trigram ‘cba’ will be mapped to its HD vector as follows:\n",
    "ρ\n",
    "1\n",
    "(Hc) \f",
    " ρ\n",
    "2\n",
    "(Hb) \f",
    " ρ\n",
    "3\n",
    "(Ha) the process of forming HD vector of an n-gram can be formalized as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc249baa-6259-49ec-9aeb-fda4e2b7e318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cab hd vector: [-1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1. -1. -1. -1. -1.  1. -1. -1.\n",
      "  1. -1. -1.  1. -1. -1. -1.  1.  1. -1. -1. -1. -1. -1.  1. -1. -1. -1.\n",
      "  1.  1.  1. -1. -1. -1.  1.  1. -1. -1. -1.  1.  1. -1.  1. -1.  1.  1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1.  1.  1.  1.  1.  1.  1.  1. -1. -1.\n",
      " -1.  1. -1. -1.  1.  1. -1.  1. -1. -1. -1. -1. -1.  1. -1. -1.  1.  1.\n",
      " -1.  1. -1. -1. -1. -1.  1. -1.  1. -1.]\n"
     ]
    }
   ],
   "source": [
    "def ngram_HD(ngram, hd_dimension):\n",
    "    ngram_hd = np.ones((hd_dimension,) )\n",
    "    for i in range(len(ngram)):\n",
    "        sym_idx = alphabet.index(ngram[i])\n",
    "        hd_vector = get_hd_vector(sym_idx, item_memory)\n",
    "        # apply i+1 times rho\n",
    "        hd_vector = rho(hd_vector, i+1)\n",
    "        ngram_hd = ngram_hd*hd_vector\n",
    "    return ngram_hd\n",
    "\n",
    "ngram_example = 'cba'\n",
    "example_output = ngram_HD(ngram_example, 100)\n",
    "print('cab hd vector:', example_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1e1c67-b15a-4c3a-a6dd-d43de19b6196",
   "metadata": {},
   "source": [
    "# mapping the whole n-gram statistics s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c2cc56a-68c3-4056-8fa1-71f81eefa796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_stat_h(ngrams, hd_dimension, n=3, alphabet_size=alphabet_size):\n",
    "\n",
    "    h = np.zeros((hd_dimension,))\n",
    "    # Initialize vector s\n",
    "    num_ngrams = alphabet_size ** n\n",
    "    s_i = np.zeros(num_ngrams)\n",
    "\n",
    "    # Create a mapping from n-gram to index in vector s\n",
    "    ngram_to_index = defaultdict(lambda: len(ngram_to_index))\n",
    "    \n",
    "    for ngram in ngrams:\n",
    "        index = ngram_to_index[ngram]\n",
    "        s_i[index] += 1\n",
    "        \n",
    "    for ngram in ngrams:    \n",
    "        ngram_hd = ngram_HD(ngram, hd_dimension)\n",
    "        h += s_i[ngram_to_index[ngram]]*ngram_hd\n",
    "    return h\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f2584de-e0bd-49df-b44d-09377fcb7513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a short sentence example n-gram statistics:\n",
      " [ 12. -22.  18.  -4. -22. -16.  18.  12.  32.  12.  -2.  -6.   4.  36.\n",
      " -22. -44. -26. -28.  42.   8.  10. -32. -20.  30. -26.   8.   6.  24.\n",
      " -16.  -6.  26.   8. -22.   8. -38.  52.  28.  18.  24.  10. -12.  -8.\n",
      "  12.  34. -48.   4. -14.  10.   4.  -2.   0.  38.  44.  -6.  18. -18.\n",
      " -18.  28. -54.  -2.  22.  14. -34. -18.  20. -30.  42. -14. -16.   0.\n",
      " -22. -24.   2. -30.  18.  -8.  18.  -2.   4.  -6. -10. -22.  34.  36.\n",
      "  30. -28.   2.  -2. -54.  12.  12.  46.  12.  54. -44.  12.  20.  24.\n",
      "  48.  60.]\n"
     ]
    }
   ],
   "source": [
    "example1_h_stat = ngram_stat_h(ngrams_example1, hd_dimension1)\n",
    "print('a short sentence example n-gram statistics:\\n', example1_h_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "249141c5-c851-4090-8861-84acc503f18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "L2 norm of the array:\n",
      "253.7794317906792\n",
      "\n",
      "Normalized array:\n",
      "[ 0.04728516 -0.08668945  0.07092773 -0.01576172 -0.08668945 -0.06304687\n",
      "  0.07092773  0.04728516  0.12609375  0.04728516 -0.00788086 -0.02364258\n",
      "  0.01576172  0.14185547 -0.08668945 -0.17337891 -0.10245117 -0.11033203\n",
      "  0.16549805  0.03152344  0.0394043  -0.12609375 -0.07880859  0.11821289\n",
      " -0.10245117  0.03152344  0.02364258  0.09457031 -0.06304687 -0.02364258\n",
      "  0.10245117  0.03152344 -0.08668945  0.03152344 -0.14973633  0.20490234\n",
      "  0.11033203  0.07092773  0.09457031  0.0394043  -0.04728516 -0.03152344\n",
      "  0.04728516  0.13397461 -0.18914062  0.01576172 -0.05516602  0.0394043\n",
      "  0.01576172 -0.00788086  0.          0.14973633  0.17337891 -0.02364258\n",
      "  0.07092773 -0.07092773 -0.07092773  0.11033203 -0.2127832  -0.00788086\n",
      "  0.08668945  0.05516602 -0.13397461 -0.07092773  0.07880859 -0.11821289\n",
      "  0.16549805 -0.05516602 -0.06304687  0.         -0.08668945 -0.09457031\n",
      "  0.00788086 -0.11821289  0.07092773 -0.03152344  0.07092773 -0.00788086\n",
      "  0.01576172 -0.02364258 -0.0394043  -0.08668945  0.13397461  0.14185547\n",
      "  0.11821289 -0.11033203  0.00788086 -0.00788086 -0.2127832   0.04728516\n",
      "  0.04728516  0.18125976  0.04728516  0.2127832  -0.17337891  0.04728516\n",
      "  0.07880859  0.09457031  0.18914062  0.23642578]\n"
     ]
    }
   ],
   "source": [
    "l2_norm_example1 = np.linalg.norm(example1_h_stat)\n",
    "\n",
    "normalized_array = example1_h_stat / l2_norm_example1\n",
    "\n",
    "print(\"\\nL2 norm of the array:\")\n",
    "print(l2_norm_example1)\n",
    "print(\"\\nNormalized array:\")\n",
    "print(normalized_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca104ade-b819-4229-b5dd-1d4f6912934f",
   "metadata": {},
   "source": [
    "# 21 languages centroid HD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1a2acba-700c-4353-a105-4fc5bee40bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        sentences = file.readlines()\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "18af28e5-9e44-4524-9126-6f61178a7d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_random_sentences(sentences, num_sentences):\n",
    "    return random.sample(sentences, min(num_sentences, len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf1ab9b7-0d4d-4252-9681-8113f31967d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './news'\n",
    "\n",
    "def HD_centroid_1(hd_dimension):\n",
    "    # Initialize item memory\n",
    "    item_memory = init_memory(alphabet_size, hd_dimension)\n",
    "\n",
    "    HD_centroids = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                preprocessed_text = preprocess(text)\n",
    "                ngrams_text = ngrams(preprocessed_text)\n",
    "                h_stat = ngram_stat_h(ngrams_text, hd_dimension)\n",
    "                # L2 normalize h\n",
    "                l2_norm = np.linalg.norm(h_stat)\n",
    "                normed_h = h_stat / l2_norm\n",
    "                HD_centroids[filename.split('_')[0]] = normed_h\n",
    "    return HD_centroids\n",
    "\n",
    "def HD_centroid(hd_dimension):\n",
    "    # Initialize item memory\n",
    "    item_memory = init_memory(alphabet_size, hd_dimension)\n",
    "\n",
    "    HD_centroids = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            centroid = []\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            sentences = read_sentences(file_path)\n",
    "            sentences = pick_random_sentences(sentences, 1000)\n",
    "            for sentence in sentences:\n",
    "                preprocessed_sentence = preprocess(sentence)\n",
    "                ngrams_sentence = ngrams(preprocessed_sentence)\n",
    "                h_stat = ngram_stat_h(ngrams_sentence, hd_dimension)\n",
    "                # L2 normalize h\n",
    "                epsilon = 1e-4\n",
    "                l2_norm = np.linalg.norm(h_stat)\n",
    "                normed_h = h_stat / (l2_norm + epsilon)\n",
    "                centroid.append(normed_h)\n",
    "            mean = np.mean(np.array(centroid), axis=0)\n",
    "            HD_centroids[filename.split('_')[0]] = mean\n",
    "            print(f'Language: {filename.split('_')[0]} HD centroid is generated')\n",
    "    return HD_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "18a48209-7f9f-4b15-9668-9d56ab8306df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language: bul HD centroid is generated\n",
      "Language: ces HD centroid is generated\n",
      "Language: dan HD centroid is generated\n",
      "Language: deu HD centroid is generated\n",
      "Language: ell HD centroid is generated\n",
      "Language: eng HD centroid is generated\n",
      "Language: est HD centroid is generated\n",
      "Language: fin HD centroid is generated\n",
      "Language: fra HD centroid is generated\n",
      "Language: hun HD centroid is generated\n",
      "Language: ita HD centroid is generated\n",
      "Language: lav HD centroid is generated\n",
      "Language: lit-lit HD centroid is generated\n",
      "Language: nld HD centroid is generated\n",
      "Language: pol HD centroid is generated\n",
      "Language: por HD centroid is generated\n",
      "Language: ron HD centroid is generated\n",
      "Language: slk HD centroid is generated\n",
      "Language: slv HD centroid is generated\n",
      "Language: spa HD centroid is generated\n",
      "Language: swe HD centroid is generated\n"
     ]
    }
   ],
   "source": [
    "HD_centroids_100dimension = HD_centroid(hd_dimension1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57f5ef1f-1eaa-4365-adf0-0cfdee30189a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary saved to HD_centroids_100dimension.pkl\n"
     ]
    }
   ],
   "source": [
    "with open('HD_centroids_100dimension.pkl', 'wb') as f:\n",
    "    pickle.dump(HD_centroids_100dimension, f)\n",
    "\n",
    "print(\"Dictionary saved to HD_centroids_100dimension.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0ea6bc47-4efd-4148-9092-fd57459fac80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italian HD cenroid vector:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-9.44645268e-04,  1.60869581e-02,  3.00033016e-02,  5.16537924e-02,\n",
       "        3.75799441e-03, -1.17678688e-02,  3.05207273e-02, -1.07287779e-02,\n",
       "       -2.97417883e-02, -1.13961939e-02,  3.99341935e-04,  3.16737991e-02,\n",
       "        6.00054670e-03,  3.95645109e-02, -6.00400523e-02, -6.88222889e-02,\n",
       "        1.60882953e-02,  3.51360092e-02,  7.20531819e-02,  8.35000324e-03,\n",
       "        1.26103393e-02, -3.49147394e-02, -1.36873093e-05, -4.61239159e-02,\n",
       "       -9.36766483e-03, -2.70180106e-02, -2.90798384e-02, -1.22021627e-02,\n",
       "       -1.60786985e-02,  6.34481904e-02, -7.87592577e-02, -1.18824795e-02,\n",
       "       -3.23104839e-02, -4.93591628e-03,  6.91565511e-03,  7.08775695e-02,\n",
       "        3.50864906e-02, -9.30714308e-03,  2.45613731e-02,  5.38381264e-02,\n",
       "        6.68466700e-02, -5.92544510e-03,  1.14969262e-03,  5.68186525e-02,\n",
       "       -1.61811860e-02,  1.16903657e-02,  8.48007083e-02,  4.52082478e-02,\n",
       "       -1.57128743e-02, -2.51552098e-02,  3.61076571e-02, -2.62413438e-02,\n",
       "        3.24842403e-02,  6.73089321e-02,  1.11158986e-02, -2.92922408e-02,\n",
       "        8.28794932e-02,  2.54348867e-02, -1.44515552e-02, -4.20523536e-02,\n",
       "        3.15157485e-02,  2.36201451e-02, -9.89430549e-03,  8.00224022e-03,\n",
       "       -3.98986329e-02, -7.45693080e-02,  1.59576034e-02,  1.29379499e-02,\n",
       "        3.18564456e-02,  7.70108493e-02,  4.08952360e-02,  1.49167706e-02,\n",
       "       -2.14004747e-02, -3.27718866e-02,  4.05135695e-02, -3.52195949e-03,\n",
       "        1.87985042e-02,  3.98322842e-02,  5.77433333e-02,  9.32531806e-03,\n",
       "       -1.03080182e-02, -1.49998215e-02, -1.88110977e-02,  5.87968452e-02,\n",
       "        1.39980437e-02,  4.16091985e-02, -1.17382018e-02, -4.57260498e-02,\n",
       "        2.78930347e-02, -4.45398151e-02, -2.49200669e-02, -3.88776887e-02,\n",
       "        4.92554384e-03,  2.68975883e-02,  2.89685945e-02, -6.78642777e-02,\n",
       "       -2.50665908e-02, -3.57793406e-02,  3.03837929e-02,  6.87244096e-02])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Italian HD cenroid vector:\\n')\n",
    "HD_centroids_100dimension['ita']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c879353-7aa0-4509-a142-e49319188f24",
   "metadata": {},
   "source": [
    "# Test phase using Europarl Parallel Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e677950b-b6f0-43c0-a04a-d60986bb58f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary from the file\n",
    "with open('HD_centroids_100dimension.pkl', 'rb') as f:\n",
    "    HD_centroids_100dimension = pickle.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f74a5ba3-810e-430a-a930-383cec09467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = './finaltest'\n",
    "classes = HD_centroids_100dimension.keys()\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "def classify(file_path, centroids, hd_dimension):\n",
    "    sentences = read_sentences(file_path)\n",
    "    sentences = pick_random_sentences(sentences, num_sentences=100)\n",
    "    confusion_matrix = {}\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        preprocessed_sentence = preprocess(sentence)\n",
    "        ngrams_sentence = ngrams(preprocessed_sentence)\n",
    "        h_stat = ngram_stat_h(ngrams_sentence, hd_dimension)\n",
    "        # L2 normalize h\n",
    "        l2_norm = np.linalg.norm(h_stat)\n",
    "        epsilon = 1e-4\n",
    "        normed_h = h_stat / (l2_norm + epsilon)\n",
    "        similarity = {}\n",
    "        for label, centroid in centroids.items():\n",
    "            similarity[label] = cosine_similarity(normed_h, centroid)\n",
    "        pred = max(similarity, key=similarity.get)\n",
    "        try:\n",
    "            confusion_matrix[pred] += 1\n",
    "        except KeyError:\n",
    "            confusion_matrix[pred] = 0\n",
    "    return confusion_matrix\n",
    "        \n",
    "    \n",
    "    \n",
    "def cosine_classify(test_path, centroids, hd_dimension):\n",
    "\n",
    "    for filename in os.listdir(test_path):\n",
    "        file_path = os.path.join(test_path, filename)\n",
    "        sentences = read_sentences(file_path)\n",
    "        sentences = pick_random_sentences(sentences, num_sentences=100)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            similarity = {}\n",
    "            preprocessed_sentence = preprocess(sentence)\n",
    "            ngrams_sentence = ngrams(preprocessed_sentence)\n",
    "            h_stat = ngram_stat_h(ngrams_sentence, hd_dimension)\n",
    "            # L2 normalize h\n",
    "            l2_norm = np.linalg.norm(h_stat)\n",
    "            epsilon = 1e-4\n",
    "            normed_h = h_stat / (l2_norm + epsilon)\n",
    "            \n",
    "            for label, centroid in centroids.items():\n",
    "                similarity[label] = cosine_similarity(normed_h, centroid)\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8ef6ed1e-86cd-4f04-83c9-1edf2f5dcd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jixia\\AppData\\Local\\Temp\\ipykernel_24428\\853623748.py:8: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return dot_product / (norm_a * norm_b)\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(test_path):\n",
    "        file_path = os.path.join(test_path, filename)\n",
    "swedish_test = classify(file_path, HD_centroids_100dimension, hd_dimension1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e98dea19-730a-417b-859b-8df1200a24f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = cosine_classify(test_path, HD_centroids_100dimension, hd_dimension1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3ea22b2b-96a2-47a0-b52f-7f73e5285c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'swe': 67,\n",
       " 'dan': 12,\n",
       " 'nld': 5,\n",
       " 'por': 0,\n",
       " 'bul': 0,\n",
       " 'deu': 0,\n",
       " 'ron': 1,\n",
       " 'lav': 0,\n",
       " 'ces': 0,\n",
       " 'fra': 0,\n",
       " 'eng': 2,\n",
       " 'spa': 0,\n",
       " 'pol': 0}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swedish_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16db8f6c-3e08-4c60-bfb6-0dce305dee8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
