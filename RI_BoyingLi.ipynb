{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 2 by Boying Li(boylil-3)"
      ],
      "metadata": {
        "id": "JooZAAlUqGc2"
      },
      "id": "JooZAAlUqGc2"
    },
    {
      "cell_type": "markdown",
      "id": "569132d3",
      "metadata": {
        "id": "569132d3"
      },
      "source": [
        "# Random Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "140bb43a",
      "metadata": {
        "id": "140bb43a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import text_functions as tf\n",
        "import nltk\n",
        "\n",
        "#@author: The first version of this code is the courtesy of Vadim Selyanik\n",
        "\n",
        "threshold = 15000 # Frequency threshold in the corpus ??\n",
        "dimension = 2000 # Dimensionality for high-dimensional vectors\n",
        "lemmatizer = nltk.WordNetLemmatizer()  # create an instance of lemmatizer\n",
        "ones_number = 2 # number of nonzero elements in randomly generated high-dimensional vectors\n",
        "window_size = 2 #number of neighboring words to consider both back and forth. In other words number of words before/after current word\n",
        "zero_vector = np.zeros(dimension)\n",
        "test_name = \"new_toefl.txt\" # file with TOEFL dataset\n",
        "data_file_name = \"lemmatized.text\" # file with the text corpus\n",
        "\n",
        "amount_dictionary = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb8e5a5a",
      "metadata": {
        "id": "fb8e5a5a"
      },
      "outputs": [],
      "source": [
        "# Count how many times each word appears in the corpus\n",
        "text_file = open(data_file_name, \"r\")\n",
        "for line in text_file:\n",
        "    if line != \"\\n\":\n",
        "        words = line.split()\n",
        "        for word in words:\n",
        "            if amount_dictionary.get(word) is None:\n",
        "                amount_dictionary[word] = 1\n",
        "            else:\n",
        "                amount_dictionary[word] += 1\n",
        "text_file.close()\n",
        "\n",
        "dictionary = {} #vocabulary and corresponing random high-dimensional vectors\n",
        "word_space = {} #embedings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad31263b",
      "metadata": {
        "id": "ad31263b",
        "outputId": "2b4a0eea-1812-42eb-d4d7-14a5b0b5bc6c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "70583"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(amount_dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ece62298",
      "metadata": {
        "id": "ece62298",
        "outputId": "fb40164b-dffc-4606-b526-570b4c13fb81"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('who', 20802)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(amount_dictionary.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f00804d",
      "metadata": {
        "id": "8f00804d"
      },
      "outputs": [],
      "source": [
        "#Create a dictionary with the assigned random high-dimensional vectors\n",
        "text_file = open(data_file_name, \"r\")\n",
        "for line in text_file: #read line in the file\n",
        "    words = line.split() # extract words from the line\n",
        "    for word in words:  # for each word\n",
        "        if dictionary.get(word) is None: # If the word was not yed added to the vocabulary\n",
        "            if amount_dictionary[word] < threshold:\n",
        "                dictionary[word] = tf.get_random_word_vector(dimension, ones_number) # assign a\n",
        "            else:\n",
        "                dictionary[word] = np.zeros(dimension) # frequent words are assigned with empty vectors. In a way they will not contribute to the word embedding\n",
        "\n",
        "text_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "265a78de",
      "metadata": {
        "id": "265a78de",
        "outputId": "cf831eb5-e3e4-4e02-c54b-3b403676d51d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vector dimension: 2000\n"
          ]
        }
      ],
      "source": [
        "sample_key, sample_value = next(iter(dictionary.items()))\n",
        "# check the vector dimension\n",
        "print('vector dimension:', len(sample_value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2537a99",
      "metadata": {
        "id": "a2537a99"
      },
      "outputs": [],
      "source": [
        "#Note that in order to save time we only create embeddings for the words needed in the TOEFL task\n",
        "\n",
        "#Find all unique words amongst TOEFL tasks and initialize their embeddings to zeros\n",
        "number_of_tests = 0\n",
        "text_file = open(test_name, \"r\") #open TOEFL tasks\n",
        "for line in text_file:\n",
        "        words = line.split()\n",
        "        words = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word, 'v'), 'n'), 'a') for word in\n",
        "                 words] # lemmatize words in the current test\n",
        "        word_space[words[0]] = np.zeros(dimension)\n",
        "        word_space[words[1]] = np.zeros(dimension)\n",
        "        word_space[words[2]] = np.zeros(dimension)\n",
        "        word_space[words[3]] = np.zeros(dimension)\n",
        "        word_space[words[4]] = np.zeros(dimension)\n",
        "        number_of_tests += 1\n",
        "text_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f82921cf",
      "metadata": {
        "id": "f82921cf",
        "outputId": "c07141e7-cde2-48cf-d495-6a96a558d208"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i = 2 line:who be the first american \n",
            "\n",
            "i = 3 line:many many year ago perhaps year ago life be very different than it be today \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Processing the text to build the embeddings\n",
        "text_file = open(data_file_name, \"r\")\n",
        "lines = [[],[],[],[]] # neighboring lines\n",
        "i = 2\n",
        "while i < 4:\n",
        "        line = \"\\n\"\n",
        "        while line == \"\\n\":\n",
        "            line = text_file.readline()\n",
        "            print(f'i = {i} line:{line}')\n",
        "        lines[i] = line.split()\n",
        "        i += 1\n",
        "\n",
        "line_total = sum(1 for line in text_file)\n",
        "\n",
        "text_file = open(data_file_name, \"r\")\n",
        "line = text_file.readline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2275de88",
      "metadata": {
        "id": "2275de88",
        "outputId": "9c7ab649-041d-4655-e51c-379803a597d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[],\n",
              " [],\n",
              " ['who', 'be', 'the', 'first', 'american'],\n",
              " ['many',\n",
              "  'many',\n",
              "  'year',\n",
              "  'ago',\n",
              "  'perhaps',\n",
              "  'year',\n",
              "  'ago',\n",
              "  'life',\n",
              "  'be',\n",
              "  'very',\n",
              "  'different',\n",
              "  'than',\n",
              "  'it',\n",
              "  'be',\n",
              "  'today']]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1de24c43",
      "metadata": {
        "id": "1de24c43",
        "outputId": "32cbb74e-8e52-4026-f061-6343d7227f2e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'who be the first american \\n'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "988d8cc0",
      "metadata": {
        "id": "988d8cc0"
      },
      "outputs": [],
      "source": [
        "def left_neigh(i):\n",
        "    k = 1\n",
        "    word_space_vector = word_space[words[i]]\n",
        "    while (i - k >= 0) and (k <= window_size): #process left neighbors of the focus word\n",
        "        word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[words[i - k]], -1))\n",
        "        k += 1\n",
        "    # Handle different situations if there was not enough neighbors on the left in the current line\n",
        "    if k <= window_size and (len(lines[1])>0):\n",
        "        if len(lines[1]) < 2:\n",
        "            if k != 1: #if one word on the left was already added\n",
        "                word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[1][0]], -1)) #update word embedding\n",
        "            else:\n",
        "                word_space[words[i]] = np.add(word_space[words[i]],\n",
        "                                              np.roll(dictionary[lines[1][0]], -1)) #update word embedding\n",
        "                word_space[words[i]] = np.add(word_space[words[i]],\n",
        "                                              np.roll(dictionary[lines[0][len(lines[0]) - 1]], -1)) #update word embedding\n",
        "        else:\n",
        "            if k != 1:\n",
        "                word_space[words[i]] = np.add(word_space[words[i]],\n",
        "                                              np.roll(dictionary[lines[1][len(lines[1]) - 1]], -1)) #update word embedding\n",
        "            else:\n",
        "                word_space[words[i]] = np.add(word_space[words[i]],\n",
        "                                              np.roll(dictionary[lines[1][len(lines[1]) - 1]], -1)) #update word embedding\n",
        "                word_space[words[i]] = np.add(word_space[words[i]],\n",
        "                                              np.roll(dictionary[lines[1][len(lines[1]) - 2]], -1)) #update word embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88b13ea4",
      "metadata": {
        "id": "88b13ea4"
      },
      "outputs": [],
      "source": [
        "def right_neigh(i):\n",
        "    k = 1\n",
        "    while (i + k < length) and (k <= window_size): #process right neighbors of the focus word\n",
        "        word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[words[i + k]], 1)) #update word embedding\n",
        "        k += 1\n",
        "    if k <= window_size:\n",
        "        if len(lines[3]) < 2:\n",
        "            if k != 1:\n",
        "                word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[3][0]], 1)) #update word embedding\n",
        "            else:\n",
        "                word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[3][0]], 1)) #update word embedding\n",
        "                word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[4][0]], 1)) #update word embedding\n",
        "        else:\n",
        "            if k != 1:\n",
        "                word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[3][0]], 1)) #update word embedding\n",
        "            else:\n",
        "                word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[3][0]], 1)) #update word embedding\n",
        "                word_space[words[i]] = np.add(word_space[words[i]],\n",
        "                                          np.roll(dictionary[lines[3][1]], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d63b6d3",
      "metadata": {
        "id": "7d63b6d3",
        "outputId": "5d6dff95-e09f-48ed-a894-22221202c19b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word space updating in progress 10%\n",
            "word space updating in progress 20%\n",
            "word space updating in progress 30%\n",
            "word space updating in progress 40%\n",
            "word space updating in progress 50%\n",
            "word space updating in progress 60%\n",
            "word space updating in progress 70%\n",
            "word space updating in progress 80%\n",
            "word space updating in progress 90%\n",
            "word space updating in progress 100%\n"
          ]
        }
      ],
      "source": [
        "progress = 0\n",
        "while line != \"\":\n",
        "        if line != \"\\n\":\n",
        "            lines.append(line.split())\n",
        "            words = lines[2]\n",
        "            length = len(words)\n",
        "            i = 0\n",
        "            while i < length:\n",
        "                if not (word_space.get(words[i]) is None):\n",
        "                    left_neigh(i)\n",
        "                    right_neigh(i)\n",
        "                i += 1\n",
        "            lines.pop(0)\n",
        "        line = text_file.readline()\n",
        "        progress += 1\n",
        "        if progress in [x*(line_total//10) for x in range(1,11)]:\n",
        "            print(f'word space updating in progress {10*progress//(line_total//10)}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ed1ca31",
      "metadata": {
        "id": "0ed1ca31",
        "outputId": "581390a0-e115-46ff-e37e-3f6d2634ff27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('enormously', array([0., 5., 0., ..., 0., 0., 0.]))"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(word_space.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8596c01c",
      "metadata": {
        "id": "8596c01c",
        "outputId": "57188be9-6616-42f8-accc-cedf36a8ce08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2000 Percentage of correct answers: 68.75%\n"
          ]
        }
      ],
      "source": [
        "#Testing of the embeddings on TOEFL\n",
        "a = 0.0 # accuracy of the encodings\n",
        "i = 0\n",
        "text_file = open(test_name, 'r')\n",
        "right_answers = 0.0 # variable for correct answers\n",
        "number_skipped_tests = 0.0 # some tests could be skipped if there are no corresponding words in the vocabulary extracted from the training corpus\n",
        "while i < number_of_tests:\n",
        "        line = text_file.readline() #read line in the file\n",
        "        words = line.split()  # extract words from the line\n",
        "        words = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word, 'v'), 'n'), 'a') for word in\n",
        "                  words]  # lemmatize words in the current test\n",
        "        try:\n",
        "\n",
        "            if not(amount_dictionary.get(words[0]) is None): # check if there word in the corpus for the query word\n",
        "                k = 1\n",
        "                while k < 5:\n",
        "                    # if amount_dictionary.get(words[k]) is None:\n",
        "                    #     word_space[words[k]] = np.random.randn(dimension)\n",
        "                    if np.array_equal(word_space[words[k]], zero_vector): # if no representation was learnt assign a random vector\n",
        "                        word_space[words[k]] = np.random.randn(dimension)\n",
        "                    k += 1\n",
        "                right_answers += tf.get_answer_mod([word_space[words[0]],word_space[words[1]],word_space[words[2]],\n",
        "                            word_space[words[3]],word_space[words[4]]]) #check if word is predicted right\n",
        "        except KeyError: # if there is no representation for the query vector than skip\n",
        "            number_skipped_tests += 1\n",
        "            print(\"skipped test: \" + str(i) + \"; Line: \" + str(words))\n",
        "        except IndexError:\n",
        "            print(i)\n",
        "            print(line)\n",
        "            print(words)\n",
        "            break\n",
        "        i += 1\n",
        "text_file.close()\n",
        "a += 100 * right_answers / number_of_tests\n",
        "print(str(dimension) + \" Percentage of correct answers: \" + str(100 * right_answers / number_of_tests) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "533960f2",
      "metadata": {
        "id": "533960f2"
      },
      "source": [
        "##  window size\n",
        "In order to handle bigger window size, the lines that handle edge(if enough neighbors on the left/right in the current line) should be ajusted."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af94b8ac",
      "metadata": {
        "id": "af94b8ac"
      },
      "source": [
        "## Dimension = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f142510",
      "metadata": {
        "id": "8f142510",
        "outputId": "cd3f6149-80d7-4563-f883-bbea1d6452b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word space updating in progress 10%\n",
            "word space updating in progress 20%\n",
            "word space updating in progress 30%\n",
            "word space updating in progress 40%\n",
            "word space updating in progress 50%\n",
            "word space updating in progress 60%\n",
            "word space updating in progress 70%\n",
            "word space updating in progress 80%\n",
            "word space updating in progress 90%\n",
            "word space updating in progress 100%\n",
            "Wall time: 31.2 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "dimension = 500\n",
        "dictionary = {} #vocabulary and corresponing random high-dimensional vectors\n",
        "word_space = {} #embedings\n",
        "#Create a dictionary with the assigned random high-dimensional vectors\n",
        "text_file = open(data_file_name, \"r\")\n",
        "for line in text_file: #read line in the file\n",
        "    words = line.split() # extract words from the line\n",
        "    for word in words:  # for each word\n",
        "        if dictionary.get(word) is None: # If the word was not yed added to the vocabulary\n",
        "            if amount_dictionary[word] < threshold:\n",
        "                dictionary[word] = tf.get_random_word_vector(dimension, ones_number) # assign a\n",
        "            else:\n",
        "                dictionary[word] = np.zeros(dimension) # frequent words are assigned with empty vectors. In a way they will not contribute to the word embedding\n",
        "\n",
        "text_file.close()\n",
        "\n",
        "#Note that in order to save time we only create embeddings for the words needed in the TOEFL task\n",
        "\n",
        "#Find all unique words amongst TOEFL tasks and initialize their embeddings to zeros\n",
        "number_of_tests = 0\n",
        "text_file = open(test_name, \"r\") #open TOEFL tasks\n",
        "for line in text_file:\n",
        "        words = line.split()\n",
        "        words = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word, 'v'), 'n'), 'a') for word in\n",
        "                 words] # lemmatize words in the current test\n",
        "        word_space[words[0]] = np.zeros(dimension)\n",
        "        word_space[words[1]] = np.zeros(dimension)\n",
        "        word_space[words[2]] = np.zeros(dimension)\n",
        "        word_space[words[3]] = np.zeros(dimension)\n",
        "        word_space[words[4]] = np.zeros(dimension)\n",
        "        number_of_tests += 1\n",
        "text_file.close()\n",
        "\n",
        "# Processing the text to build the embeddings\n",
        "text_file = open(data_file_name, \"r\")\n",
        "lines = [[],[],[],[]] # neighboring lines\n",
        "i = 2\n",
        "while i < 4:\n",
        "        line = \"\\n\"\n",
        "        while line == \"\\n\":\n",
        "            line = text_file.readline()\n",
        "\n",
        "        lines[i] = line.split()\n",
        "        i += 1\n",
        "\n",
        "line_total = sum(1 for line in text_file)\n",
        "\n",
        "text_file = open(data_file_name, \"r\")\n",
        "line = text_file.readline()\n",
        "\n",
        "progress = 0\n",
        "while line != \"\":\n",
        "        if line != \"\\n\":\n",
        "            lines.append(line.split())\n",
        "            words = lines[2]\n",
        "            length = len(words)\n",
        "            i = 0\n",
        "            while i < length:\n",
        "                if not (word_space.get(words[i]) is None):\n",
        "                    left_neigh(i)\n",
        "                    right_neigh(i)\n",
        "                i += 1\n",
        "            lines.pop(0)\n",
        "        line = text_file.readline()\n",
        "        progress += 1\n",
        "        if progress in [x*(line_total//10) for x in range(1,11)]:\n",
        "            print(f'word space updating in progress {10*progress//(line_total//10)}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abf1f15f",
      "metadata": {
        "id": "abf1f15f",
        "outputId": "0b90df99-8f4e-4ebb-a345-da733ffbdd3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vector dimension: 500\n"
          ]
        }
      ],
      "source": [
        "_, vector = next(iter(word_space.items()))\n",
        "print('vector dimension:', len(vector))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55863363",
      "metadata": {
        "id": "55863363",
        "outputId": "8366a60a-c8b0-42fd-abcc-6e151add71c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "500 Percentage of correct answers: 53.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Bella\\anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:620: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ]
        }
      ],
      "source": [
        "#Testing of the embeddings on TOEFL\n",
        "a = 0.0 # accuracy of the encodings\n",
        "i = 0\n",
        "text_file = open(test_name, 'r')\n",
        "right_answers = 0.0 # variable for correct answers\n",
        "number_skipped_tests = 0.0 # some tests could be skipped if there are no corresponding words in the vocabulary extracted from the training corpus\n",
        "while i < number_of_tests:\n",
        "        line = text_file.readline() #read line in the file\n",
        "        words = line.split()  # extract words from the line\n",
        "        words = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word, 'v'), 'n'), 'a') for word in\n",
        "                  words]  # lemmatize words in the current test\n",
        "        try:\n",
        "\n",
        "            if not(amount_dictionary.get(words[0]) is None): # check if there word in the corpus for the query word\n",
        "                k = 1\n",
        "                while k < 5:\n",
        "                    # if amount_dictionary.get(words[k]) is None:\n",
        "                    #     word_space[words[k]] = np.random.randn(dimension)\n",
        "                    if np.array_equal(word_space[words[k]], zero_vector): # if no representation was learnt assign a random vector\n",
        "                        word_space[words[k]] = np.random.randn(dimension)\n",
        "                    k += 1\n",
        "                right_answers += tf.get_answer_mod([word_space[words[0]],word_space[words[1]],word_space[words[2]],\n",
        "                            word_space[words[3]],word_space[words[4]]]) #check if word is predicted right\n",
        "        except KeyError: # if there is no representation for the query vector than skip\n",
        "            number_skipped_tests += 1\n",
        "            print(\"skipped test: \" + str(i) + \"; Line: \" + str(words))\n",
        "        except IndexError:\n",
        "            print(i)\n",
        "            print(line)\n",
        "            print(words)\n",
        "            break\n",
        "        i += 1\n",
        "text_file.close()\n",
        "a += 100 * right_answers / number_of_tests\n",
        "print(str(dimension) + \" Percentage of correct answers: \" + str(100 * right_answers / number_of_tests) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8952c5e",
      "metadata": {
        "id": "a8952c5e"
      },
      "source": [
        "## Dimension = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5c3f343",
      "metadata": {
        "id": "d5c3f343",
        "outputId": "842e1fe9-c384-418f-cb07-5873a86a3524"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word space updating in progress 10%\n",
            "word space updating in progress 20%\n",
            "word space updating in progress 30%\n",
            "word space updating in progress 40%\n",
            "word space updating in progress 50%\n",
            "word space updating in progress 60%\n",
            "word space updating in progress 70%\n",
            "word space updating in progress 80%\n",
            "word space updating in progress 90%\n",
            "word space updating in progress 100%\n",
            "Wall time: 1min 8s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "dimension = 10000\n",
        "dictionary = {} #vocabulary and corresponing random high-dimensional vectors\n",
        "word_space = {} #embedings\n",
        "#Create a dictionary with the assigned random high-dimensional vectors\n",
        "text_file = open(data_file_name, \"r\")\n",
        "for line in text_file: #read line in the file\n",
        "    words = line.split() # extract words from the line\n",
        "    for word in words:  # for each word\n",
        "        if dictionary.get(word) is None: # If the word was not yed added to the vocabulary\n",
        "            if amount_dictionary[word] < threshold:\n",
        "                dictionary[word] = tf.get_random_word_vector(dimension, ones_number) # assign a\n",
        "            else:\n",
        "                dictionary[word] = np.zeros(dimension) # frequent words are assigned with empty vectors. In a way they will not contribute to the word embedding\n",
        "\n",
        "text_file.close()\n",
        "\n",
        "#Note that in order to save time we only create embeddings for the words needed in the TOEFL task\n",
        "\n",
        "#Find all unique words amongst TOEFL tasks and initialize their embeddings to zeros\n",
        "number_of_tests = 0\n",
        "text_file = open(test_name, \"r\") #open TOEFL tasks\n",
        "for line in text_file:\n",
        "        words = line.split()\n",
        "        words = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word, 'v'), 'n'), 'a') for word in\n",
        "                 words] # lemmatize words in the current test\n",
        "        word_space[words[0]] = np.zeros(dimension)\n",
        "        word_space[words[1]] = np.zeros(dimension)\n",
        "        word_space[words[2]] = np.zeros(dimension)\n",
        "        word_space[words[3]] = np.zeros(dimension)\n",
        "        word_space[words[4]] = np.zeros(dimension)\n",
        "        number_of_tests += 1\n",
        "text_file.close()\n",
        "\n",
        "# Processing the text to build the embeddings\n",
        "text_file = open(data_file_name, \"r\")\n",
        "lines = [[],[],[],[]] # neighboring lines\n",
        "i = 2\n",
        "while i < 4:\n",
        "        line = \"\\n\"\n",
        "        while line == \"\\n\":\n",
        "            line = text_file.readline()\n",
        "\n",
        "        lines[i] = line.split()\n",
        "        i += 1\n",
        "\n",
        "line_total = sum(1 for line in text_file)\n",
        "\n",
        "text_file = open(data_file_name, \"r\")\n",
        "line = text_file.readline()\n",
        "\n",
        "progress = 0\n",
        "while line != \"\":\n",
        "        if line != \"\\n\":\n",
        "            lines.append(line.split())\n",
        "            words = lines[2]\n",
        "            length = len(words)\n",
        "            i = 0\n",
        "            while i < length:\n",
        "                if not (word_space.get(words[i]) is None):\n",
        "                    left_neigh(i)\n",
        "                    right_neigh(i)\n",
        "                i += 1\n",
        "            lines.pop(0)\n",
        "        line = text_file.readline()\n",
        "        progress += 1\n",
        "        if progress in [x*(line_total//10) for x in range(1,11)]:\n",
        "            print(f'word space updating in progress {10*progress//(line_total//10)}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07e8a332",
      "metadata": {
        "id": "07e8a332",
        "outputId": "490bad92-e3dd-43ab-8db2-569d6f4e8490"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000 Percentage of correct answers: 75.0%\n"
          ]
        }
      ],
      "source": [
        "#Testing of the embeddings on TOEFL\n",
        "a = 0.0 # accuracy of the encodings\n",
        "i = 0\n",
        "text_file = open(test_name, 'r')\n",
        "right_answers = 0.0 # variable for correct answers\n",
        "number_skipped_tests = 0.0 # some tests could be skipped if there are no corresponding words in the vocabulary extracted from the training corpus\n",
        "while i < number_of_tests:\n",
        "        line = text_file.readline() #read line in the file\n",
        "        words = line.split()  # extract words from the line\n",
        "        words = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word, 'v'), 'n'), 'a') for word in\n",
        "                  words]  # lemmatize words in the current test\n",
        "        try:\n",
        "\n",
        "            if not(amount_dictionary.get(words[0]) is None): # check if there word in the corpus for the query word\n",
        "                k = 1\n",
        "                while k < 5:\n",
        "                    # if amount_dictionary.get(words[k]) is None:\n",
        "                    #     word_space[words[k]] = np.random.randn(dimension)\n",
        "                    if np.array_equal(word_space[words[k]], zero_vector): # if no representation was learnt assign a random vector\n",
        "                        word_space[words[k]] = np.random.randn(dimension)\n",
        "                    k += 1\n",
        "                right_answers += tf.get_answer_mod([word_space[words[0]],word_space[words[1]],word_space[words[2]],\n",
        "                            word_space[words[3]],word_space[words[4]]]) #check if word is predicted right\n",
        "        except KeyError: # if there is no representation for the query vector than skip\n",
        "            number_skipped_tests += 1\n",
        "            print(\"skipped test: \" + str(i) + \"; Line: \" + str(words))\n",
        "        except IndexError:\n",
        "            print(i)\n",
        "            print(line)\n",
        "            print(words)\n",
        "            break\n",
        "        i += 1\n",
        "text_file.close()\n",
        "a += 100 * right_answers / number_of_tests\n",
        "print(str(dimension) + \" Percentage of correct answers: \" + str(100 * right_answers / number_of_tests) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd6bf65a",
      "metadata": {
        "id": "bd6bf65a"
      },
      "source": [
        "Summary: In this lab, I add a simple function that print out the training progress every 10% to give an indication that training process is on. I have tried dimension of 500(accuracy=53.75%), 2000(accuracy=68.75%) and 10000(accuracy=75%). Higher dimension gives higher accuracy, however increasing dimensionality from 2000 to 10000 give not that much accuracy improvement. Compared to word2vec embedding, the dimension needed in RI is higher and also the vector is more sparse. If I compare both 500 dimension embedding, word2vec is much slower than RI, while RI forms representations only for the subset of the vocabulary."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}