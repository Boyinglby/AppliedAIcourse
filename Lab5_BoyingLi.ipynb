{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab5 by Boying Li(boylil.3)"
      ],
      "metadata": {
        "id": "kkz5U5ROnjSV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gq-R2Oi4hwFi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUZHVb9jhwFj"
      },
      "outputs": [],
      "source": [
        "#functions of non-linear activations\n",
        "def f_sigmoid(X, deriv=False):\n",
        "    if not deriv:\n",
        "        return 1 / (1 + np.exp(-X))\n",
        "    else:\n",
        "        return f_sigmoid(X)*(1 - f_sigmoid(X))\n",
        "\n",
        "\n",
        "def f_softmax(X):\n",
        "    Z = np.sum(np.exp(X), axis=1)\n",
        "    Z = Z.reshape(Z.shape[0], 1)\n",
        "    return np.exp(X) / Z\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fq5OyZfLhwFj"
      },
      "outputs": [],
      "source": [
        "def exit_with_err(err_str):\n",
        "    print(err_str, file=sys.stderr)\n",
        "    sys.exit(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owmXpTVMhwFk"
      },
      "outputs": [],
      "source": [
        "#Functionality of a single hidden layer\n",
        "class Layer:\n",
        "    def __init__(self, size, batch_size, is_input=False, is_output=False,\n",
        "                 activation=f_sigmoid):\n",
        "        self.is_input = is_input\n",
        "        self.is_output = is_output\n",
        "\n",
        "        # Z is the matrix that holds output values\n",
        "        self.Z = np.zeros((batch_size, size[0]))\n",
        "        # The activation function is an externally defined function (with a\n",
        "        # derivative) that is stored here\n",
        "        self.activation = activation\n",
        "\n",
        "        # W is the outgoing weight matrix for this layer\n",
        "        self.W = None\n",
        "        # S is the matrix that holds the inputs to this layer\n",
        "        self.S = None\n",
        "        # D is the matrix that holds the deltas for this layer\n",
        "        self.D = None\n",
        "        # Fp is the matrix that holds the derivatives of the activation function\n",
        "        self.Fp = None\n",
        "\n",
        "        if not is_input:\n",
        "            self.S = np.zeros((batch_size, size[0]))\n",
        "            self.D = np.zeros((batch_size, size[0]))\n",
        "\n",
        "        if not is_output:\n",
        "            self.W = np.random.normal(size=size, scale=1E-4)\n",
        "\n",
        "        if not is_input and not is_output:\n",
        "            self.Fp = np.zeros((size[0], batch_size))\n",
        "\n",
        "    def forward_propagate(self):\n",
        "        if self.is_input:\n",
        "            return self.Z.dot(self.W)\n",
        "\n",
        "        self.Z = self.activation(self.S)\n",
        "        if self.is_output:\n",
        "            return self.Z\n",
        "        else:\n",
        "            # For hidden layers, we add the bias values here\n",
        "            self.Z = np.append(self.Z, np.ones((self.Z.shape[0], 1)), axis=1)\n",
        "            self.Fp = self.activation(self.S, deriv=True).T\n",
        "            return self.Z.dot(self.W)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUhGkDuvhwFk"
      },
      "outputs": [],
      "source": [
        "class MultiLayerPerceptron:\n",
        "    def __init__(self, layer_config, batch_size=100):\n",
        "        self.layers = []\n",
        "        self.num_layers = len(layer_config)\n",
        "        self.minibatch_size = batch_size\n",
        "\n",
        "        for i in range(self.num_layers-1):\n",
        "            if i == 0:\n",
        "                print (\"Initializing input layer with size {0}.\".format(layer_config[i]))\n",
        "                # Here, we add an additional unit at the input for the bias\n",
        "                # weight.\n",
        "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
        "                                         batch_size,\n",
        "                                         is_input=True))\n",
        "            else:\n",
        "                print (\"Initializing hidden layer with size {0}.\".format(layer_config[i]))\n",
        "                # Here we add an additional unit in the hidden layers for the\n",
        "                # bias weight.\n",
        "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
        "                                         batch_size,\n",
        "                                         activation=f_sigmoid))\n",
        "\n",
        "        print (\"Initializing output layer with size {0}.\".format(layer_config[-1]))\n",
        "        self.layers.append(Layer([layer_config[-1], None],\n",
        "                                 batch_size,\n",
        "                                 is_output=True,\n",
        "                                 activation=f_softmax))\n",
        "        print (\"Done!\")\n",
        "\n",
        "    def forward_propagate(self, data):\n",
        "        # We need to be sure to add bias values to the input\n",
        "        self.layers[0].Z = np.append(data, np.ones((data.shape[0], 1)), axis=1)\n",
        "\n",
        "        for i in range(self.num_layers-1):\n",
        "            self.layers[i+1].S = self.layers[i].forward_propagate()\n",
        "        return self.layers[-1].forward_propagate()\n",
        "\n",
        "    def backpropagate(self, yhat, labels):\n",
        "\n",
        "        #-----------------------------------------------------------\n",
        "        # exit_with_err(\"FIND ME IN THE CODE, What is computed in the next line of code?\\n\")\n",
        "        # compute the output layer error\n",
        "        #-----------------------------------------------------------\n",
        "\n",
        "        self.layers[-1].D = (yhat - labels).T\n",
        "        for i in range(self.num_layers-2, 0, -1):\n",
        "            # We do not calculate deltas for the bias values\n",
        "            W_nobias = self.layers[i].W[0:-1, :]\n",
        "            #------------------------------------------------------------\n",
        "            #exit_with_err(\"FIND ME IN THE CODE, What does this 'for' loop do?\\n\")\n",
        "            #backpropagate the error layer by layer\n",
        "            #---------------------------------------------------------\n",
        "\n",
        "            self.layers[i].D = W_nobias.dot(self.layers[i+1].D) * self.layers[i].Fp\n",
        "\n",
        "    def update_weights(self, eta):\n",
        "        for i in range(0, self.num_layers-1):\n",
        "            W_grad = -eta*(self.layers[i+1].D.dot(self.layers[i].Z)).T\n",
        "            self.layers[i].W += W_grad\n",
        "\n",
        "    def evaluate(self, train_data, train_labels, test_data, test_labels,\n",
        "                 num_epochs=70, eta=0.05, eval_train=False, eval_test=True):\n",
        "\n",
        "        N_train = len(train_labels)*len(train_labels[0])\n",
        "        N_test = len(test_labels)*len(test_labels[0])\n",
        "\n",
        "        print (\"Training for {0} epochs...\".format(num_epochs))\n",
        "        for t in range(0, num_epochs):\n",
        "            out_str = \"[{0:4d}] \".format(t)\n",
        "\n",
        "            for b_data, b_labels in zip(train_data, train_labels):\n",
        "                output = self.forward_propagate(b_data)\n",
        "                self.backpropagate(output, b_labels)\n",
        "                #-----------------------------------------------\n",
        "                #exit_with_err(\"FIND ME IN THE CODE, How does weight update is implemented? What is eta?\\n\")\n",
        "                # eta is learning rate, in function update_weights() the gradient of weight is computed by\n",
        "                # the dot product of the delta(error) and the current output\n",
        "                #------------------------------------------------\n",
        "\n",
        "                self.update_weights(eta=eta)\n",
        "\n",
        "            if eval_train:\n",
        "                errs = 0\n",
        "                for b_data, b_labels in zip(train_data, train_labels):\n",
        "                    output = self.forward_propagate(b_data)\n",
        "                    yhat = np.argmax(output, axis=1)\n",
        "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
        "\n",
        "                out_str = (\"{0} Training error: {1:.5f}\".format(out_str,\n",
        "                                                           float(errs)/N_train))\n",
        "\n",
        "            if eval_test:\n",
        "                errs = 0\n",
        "                for b_data, b_labels in zip(test_data, test_labels):\n",
        "                    output = self.forward_propagate(b_data)\n",
        "                    yhat = np.argmax(output, axis=1)\n",
        "\n",
        "                    #-----------add accuracy---------------------\n",
        "                    acc = np.sum(yhat == np.argmax(b_labels,axis=1))/len(b_labels)\n",
        "                    #--------------------------------------------\n",
        "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
        "\n",
        "                out_str = (\"{0} Test error: {1:.5f}\").format(out_str,\n",
        "                                                       float(errs)/N_test) + f\" Test accuracy: {acc}\"\n",
        "\n",
        "\n",
        "            print(out_str)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fubKoe8hwFk"
      },
      "outputs": [],
      "source": [
        "def label_to_bit_vector(labels, nbits):\n",
        "    bit_vector = np.zeros((labels.shape[0], nbits))\n",
        "    for i in range(labels.shape[0]):\n",
        "        bit_vector[i, labels[i]] = 1.0\n",
        "\n",
        "    return bit_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9UJQLk2hwFl"
      },
      "outputs": [],
      "source": [
        "def create_batches(data, labels, batch_size, create_bit_vector=False):\n",
        "    N = data.shape[0]\n",
        "    print (\"Batch size {0}, the number of examples {1}.\".format(batch_size,N))\n",
        "\n",
        "    if N % batch_size != 0:\n",
        "        print (\"Warning in create_minibatches(): Batch size {0} does not \" \\\n",
        "              \"evenly divide the number of examples {1}.\".format(batch_size,N))\n",
        "    chunked_data = []\n",
        "    chunked_labels = []\n",
        "    idx = 0\n",
        "    while idx + batch_size <= N:\n",
        "        chunked_data.append(data[idx:idx+batch_size, :])\n",
        "        if not create_bit_vector:\n",
        "            chunked_labels.append(labels[idx:idx+batch_size])\n",
        "        else:\n",
        "            bit_vector = label_to_bit_vector(labels[idx:idx+batch_size], 10)\n",
        "            chunked_labels.append(bit_vector)\n",
        "\n",
        "        idx += batch_size\n",
        "\n",
        "    return chunked_data, chunked_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcKOfeaBhwFl"
      },
      "outputs": [],
      "source": [
        "def prepare_for_backprop(batch_size, Train_images, Train_labels, Valid_images, Valid_labels):\n",
        "\n",
        "    print (\"Creating data...\")\n",
        "    batched_train_data, batched_train_labels = create_batches(Train_images, Train_labels,\n",
        "                                              batch_size,\n",
        "                                              create_bit_vector=True)\n",
        "    batched_valid_data, batched_valid_labels = create_batches(Valid_images, Valid_labels,\n",
        "                                              batch_size,\n",
        "                                              create_bit_vector=True)\n",
        "    print (\"Done!\")\n",
        "\n",
        "\n",
        "    return batched_train_data, batched_train_labels,  batched_valid_data, batched_valid_labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JXzxao_hwFl"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2azEzk9AhwFl",
        "outputId": "8dafc6ff-105d-401f-da3f-f8cc2f26e40e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ]
        }
      ],
      "source": [
        "(Xtr, Ltr), (X_test, L_test)=mnist.load_data()\n",
        "\n",
        "Xtr = Xtr.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)\n",
        "Xtr = Xtr.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "Xtr /= 255\n",
        "X_test /= 255\n",
        "print(Xtr.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1.1\n",
        "a. the idea of backprop is that we get the loss/how wrong the output compared with the ground truth from the output layer(first forward pass), and we get how each layer contribute to the loss backwards, using chain rule to compute the gradient of loss with respect to weights.\n",
        "\n",
        "b. softmax() is commonly used in multiclass classification problem, it outputs the probabilities, so in the code block above we use np.argmax() to find the class that has the max probability as the predicted class.\n",
        "\n",
        "c. sigmoid (vanishing gradient problem, useful in binary classification output), tanh(vanishing gradient problem), relu (dying relu problem) and the variant leaky relu, softmax(useful in multiple classification)\n",
        "\n",
        "d. (1) compute the output layer error (2) the loop is backpropagate the error layer by layer (3) eta is learning rate, in function update_weights() the gradient of weight is computed by the dot product of the delta(error) and the current output. What I am not sure the delta calcualtion (yhat - labels), is it coming from MSE's deriative and ignore the magnitude 2???\n",
        "\n"
      ],
      "metadata": {
        "id": "_-ZfLDNgJef9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1.2 epochs = 70 and learning rate =0.05"
      ],
      "metadata": {
        "id": "yNr9jw_wJSM9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8yNEuqwhwFl",
        "outputId": "2425946d-8b89-4a1b-bbf5-f53a3519f572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating data...\n",
            "Batch size 100, the number of examples 60000.\n",
            "Batch size 100, the number of examples 10000.\n",
            "Done!\n",
            "Initializing input layer with size 784.\n",
            "Initializing hidden layer with size 100.\n",
            "Initializing hidden layer with size 100.\n",
            "Initializing output layer with size 10.\n",
            "Done!\n",
            "Training for 70 epochs...\n",
            "[   0]  Training error: 0.48887 Test error: 0.49390 Test accuracy: 0.6\n",
            "[   1]  Training error: 0.10420 Test error: 0.10500 Test accuracy: 0.92\n",
            "[   2]  Training error: 0.08580 Test error: 0.08670 Test accuracy: 0.91\n",
            "[   3]  Training error: 0.04715 Test error: 0.05230 Test accuracy: 0.92\n",
            "[   4]  Training error: 0.03990 Test error: 0.04480 Test accuracy: 0.96\n",
            "[   5]  Training error: 0.03695 Test error: 0.04600 Test accuracy: 0.97\n",
            "[   6]  Training error: 0.02850 Test error: 0.03740 Test accuracy: 0.98\n",
            "[   7]  Training error: 0.02552 Test error: 0.03520 Test accuracy: 0.98\n",
            "[   8]  Training error: 0.02415 Test error: 0.03750 Test accuracy: 0.97\n",
            "[   9]  Training error: 0.02252 Test error: 0.03480 Test accuracy: 0.97\n",
            "[  10]  Training error: 0.01672 Test error: 0.03230 Test accuracy: 0.97\n",
            "[  11]  Training error: 0.02297 Test error: 0.03700 Test accuracy: 0.97\n",
            "[  12]  Training error: 0.01777 Test error: 0.03200 Test accuracy: 0.99\n",
            "[  13]  Training error: 0.02155 Test error: 0.03780 Test accuracy: 0.97\n",
            "[  14]  Training error: 0.01797 Test error: 0.03490 Test accuracy: 0.98\n",
            "[  15]  Training error: 0.01535 Test error: 0.03440 Test accuracy: 0.98\n",
            "[  16]  Training error: 0.01220 Test error: 0.03200 Test accuracy: 0.97\n",
            "[  17]  Training error: 0.01458 Test error: 0.03480 Test accuracy: 0.98\n",
            "[  18]  Training error: 0.01717 Test error: 0.03550 Test accuracy: 0.99\n",
            "[  19]  Training error: 0.01502 Test error: 0.03570 Test accuracy: 0.99\n",
            "[  20]  Training error: 0.01138 Test error: 0.03100 Test accuracy: 0.98\n",
            "[  21]  Training error: 0.01290 Test error: 0.03350 Test accuracy: 0.99\n",
            "[  22]  Training error: 0.01017 Test error: 0.03130 Test accuracy: 0.99\n",
            "[  23]  Training error: 0.01068 Test error: 0.03190 Test accuracy: 0.99\n",
            "[  24]  Training error: 0.01152 Test error: 0.03370 Test accuracy: 0.97\n",
            "[  25]  Training error: 0.00822 Test error: 0.03040 Test accuracy: 0.98\n",
            "[  26]  Training error: 0.00818 Test error: 0.03190 Test accuracy: 0.98\n",
            "[  27]  Training error: 0.00662 Test error: 0.02840 Test accuracy: 0.99\n",
            "[  28]  Training error: 0.00963 Test error: 0.03270 Test accuracy: 0.97\n",
            "[  29]  Training error: 0.00882 Test error: 0.03280 Test accuracy: 0.99\n",
            "[  30]  Training error: 0.01058 Test error: 0.03280 Test accuracy: 0.98\n",
            "[  31]  Training error: 0.01078 Test error: 0.03350 Test accuracy: 0.99\n",
            "[  32]  Training error: 0.01223 Test error: 0.03720 Test accuracy: 0.98\n",
            "[  33]  Training error: 0.00840 Test error: 0.03150 Test accuracy: 0.99\n",
            "[  34]  Training error: 0.00832 Test error: 0.03570 Test accuracy: 0.98\n",
            "[  35]  Training error: 0.00678 Test error: 0.03180 Test accuracy: 0.98\n",
            "[  36]  Training error: 0.01102 Test error: 0.03570 Test accuracy: 0.97\n",
            "[  37]  Training error: 0.00662 Test error: 0.03160 Test accuracy: 0.99\n",
            "[  38]  Training error: 0.00503 Test error: 0.03180 Test accuracy: 0.97\n",
            "[  39]  Training error: 0.00347 Test error: 0.02960 Test accuracy: 0.99\n",
            "[  40]  Training error: 0.00322 Test error: 0.02820 Test accuracy: 0.99\n",
            "[  41]  Training error: 0.00365 Test error: 0.03100 Test accuracy: 0.98\n",
            "[  42]  Training error: 0.00285 Test error: 0.03010 Test accuracy: 0.98\n",
            "[  43]  Training error: 0.00133 Test error: 0.02840 Test accuracy: 0.99\n",
            "[  44]  Training error: 0.00060 Test error: 0.02750 Test accuracy: 0.98\n",
            "[  45]  Training error: 0.00035 Test error: 0.02750 Test accuracy: 0.98\n",
            "[  46]  Training error: 0.00017 Test error: 0.02770 Test accuracy: 0.99\n",
            "[  47]  Training error: 0.00010 Test error: 0.02760 Test accuracy: 0.99\n",
            "[  48]  Training error: 0.00008 Test error: 0.02800 Test accuracy: 0.99\n",
            "[  49]  Training error: 0.00005 Test error: 0.02800 Test accuracy: 0.99\n",
            "[  50]  Training error: 0.00005 Test error: 0.02820 Test accuracy: 0.99\n",
            "[  51]  Training error: 0.00005 Test error: 0.02820 Test accuracy: 0.99\n",
            "[  52]  Training error: 0.00003 Test error: 0.02840 Test accuracy: 0.99\n",
            "[  53]  Training error: 0.00002 Test error: 0.02840 Test accuracy: 0.99\n",
            "[  54]  Training error: 0.00002 Test error: 0.02840 Test accuracy: 0.99\n",
            "[  55]  Training error: 0.00002 Test error: 0.02820 Test accuracy: 0.99\n",
            "[  56]  Training error: 0.00002 Test error: 0.02820 Test accuracy: 0.99\n",
            "[  57]  Training error: 0.00002 Test error: 0.02800 Test accuracy: 0.99\n",
            "[  58]  Training error: 0.00002 Test error: 0.02790 Test accuracy: 0.99\n",
            "[  59]  Training error: 0.00002 Test error: 0.02790 Test accuracy: 0.99\n",
            "[  60]  Training error: 0.00002 Test error: 0.02800 Test accuracy: 0.99\n",
            "[  61]  Training error: 0.00002 Test error: 0.02800 Test accuracy: 0.99\n",
            "[  62]  Training error: 0.00002 Test error: 0.02800 Test accuracy: 0.99\n",
            "[  63]  Training error: 0.00002 Test error: 0.02800 Test accuracy: 0.99\n",
            "[  64]  Training error: 0.00002 Test error: 0.02790 Test accuracy: 0.99\n",
            "[  65]  Training error: 0.00002 Test error: 0.02800 Test accuracy: 0.99\n",
            "[  66]  Training error: 0.00002 Test error: 0.02810 Test accuracy: 0.99\n",
            "[  67]  Training error: 0.00002 Test error: 0.02820 Test accuracy: 0.99\n",
            "[  68]  Training error: 0.00002 Test error: 0.02810 Test accuracy: 0.99\n",
            "[  69]  Training error: 0.00000 Test error: 0.02800 Test accuracy: 0.99\n",
            "Done:)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "batch_size=100;\n",
        "\n",
        "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
        "\n",
        "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
        "\n",
        "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,\n",
        "             eval_train=True)\n",
        "\n",
        "print(\"Done:)\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: We can see the overfitting quite soon as the test loss is going flat while the training loss is still decreasing and less than the test loss"
      ],
      "metadata": {
        "id": "wTbAzY9uqOZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1.3 Learning rate =0.5 and 0.005"
      ],
      "metadata": {
        "id": "RBAMZwvMKinT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5SfNccjYhwFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed814922-73d4-4953-cd65-40db7d722010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating data...\n",
            "Batch size 100, the number of examples 60000.\n",
            "Batch size 100, the number of examples 10000.\n",
            "Done!\n",
            "Initializing input layer with size 784.\n",
            "Initializing hidden layer with size 100.\n",
            "Initializing hidden layer with size 100.\n",
            "Initializing output layer with size 10.\n",
            "Done!\n",
            "Training for 70 epochs...\n",
            "[   0]  Training error: 0.89782 Test error: 0.89900 Test accuracy: 0.1\n",
            "[   1]  Training error: 0.88763 Test error: 0.88650 Test accuracy: 0.12\n",
            "[   2]  Training error: 0.90070 Test error: 0.89680 Test accuracy: 0.09\n",
            "[   3]  Training error: 0.90248 Test error: 0.90260 Test accuracy: 0.1\n",
            "[   4]  Training error: 0.90248 Test error: 0.90260 Test accuracy: 0.1\n",
            "[   5]  Training error: 0.90248 Test error: 0.90260 Test accuracy: 0.1\n",
            "[   6]  Training error: 0.90263 Test error: 0.90180 Test accuracy: 0.12\n",
            "[   7]  Training error: 0.90263 Test error: 0.90180 Test accuracy: 0.12\n",
            "[   8]  Training error: 0.90137 Test error: 0.90420 Test accuracy: 0.12\n",
            "[   9]  Training error: 0.90263 Test error: 0.90180 Test accuracy: 0.12\n",
            "[  10]  Training error: 0.90137 Test error: 0.90420 Test accuracy: 0.12\n",
            "[  11]  Training error: 0.89558 Test error: 0.89720 Test accuracy: 0.11\n",
            "[  12]  Training error: 0.89782 Test error: 0.89900 Test accuracy: 0.1\n",
            "[  13]  Training error: 0.89782 Test error: 0.89900 Test accuracy: 0.1\n",
            "[  14]  Training error: 0.90085 Test error: 0.89910 Test accuracy: 0.07\n",
            "[  15]  Training error: 0.89782 Test error: 0.89900 Test accuracy: 0.1\n",
            "[  16]  Training error: 0.90085 Test error: 0.89910 Test accuracy: 0.07\n",
            "[  17]  Training error: 0.90128 Test error: 0.90200 Test accuracy: 0.11\n",
            "[  18]  Training error: 0.90137 Test error: 0.90420 Test accuracy: 0.12\n",
            "[  19]  Training error: 0.88763 Test error: 0.88650 Test accuracy: 0.12\n",
            "[  20]  Training error: 0.90070 Test error: 0.89680 Test accuracy: 0.09\n",
            "[  21]  Training error: 0.90965 Test error: 0.91080 Test accuracy: 0.06\n",
            "[  22]  Training error: 0.89782 Test error: 0.89900 Test accuracy: 0.1\n",
            "[  23]  Training error: 0.89558 Test error: 0.89720 Test accuracy: 0.11\n",
            "[  24]  Training error: 0.89782 Test error: 0.89900 Test accuracy: 0.1\n",
            "[  25]  Training error: 0.90085 Test error: 0.89910 Test accuracy: 0.07\n",
            "[  26]  Training error: 0.88763 Test error: 0.88650 Test accuracy: 0.12\n",
            "[  27]  Training error: 0.90128 Test error: 0.90200 Test accuracy: 0.11\n",
            "[  28]  Training error: 0.90085 Test error: 0.89910 Test accuracy: 0.07\n",
            "[  29]  Training error: 0.90248 Test error: 0.90260 Test accuracy: 0.1\n",
            "[  30]  Training error: 0.89782 Test error: 0.89900 Test accuracy: 0.1\n",
            "[  31]  Training error: 0.90085 Test error: 0.89910 Test accuracy: 0.07\n",
            "[  32]  Training error: 0.90965 Test error: 0.91080 Test accuracy: 0.06\n",
            "[  33]  Training error: 0.90128 Test error: 0.90200 Test accuracy: 0.11\n",
            "[  34]  Training error: 0.90137 Test error: 0.90420 Test accuracy: 0.12\n",
            "[  35]  Training error: 0.90248 Test error: 0.90260 Test accuracy: 0.1\n",
            "[  36]  Training error: 0.90070 Test error: 0.89680 Test accuracy: 0.09\n",
            "[  37]  Training error: 0.90137 Test error: 0.90420 Test accuracy: 0.12\n",
            "[  38]  Training error: 0.90263 Test error: 0.90180 Test accuracy: 0.12\n",
            "[  39]  Training error: 0.89782 Test error: 0.89900 Test accuracy: 0.1\n",
            "[  40]  Training error: 0.90085 Test error: 0.89910 Test accuracy: 0.07\n",
            "[  41]  Training error: 0.90263 Test error: 0.90180 Test accuracy: 0.12\n",
            "[  42]  Training error: 0.89558 Test error: 0.89720 Test accuracy: 0.11\n",
            "[  43]  Training error: 0.90248 Test error: 0.90260 Test accuracy: 0.1\n",
            "[  44]  Training error: 0.90248 Test error: 0.90260 Test accuracy: 0.1\n",
            "[  45]  Training error: 0.90965 Test error: 0.91080 Test accuracy: 0.06\n",
            "[  46]  Training error: 0.90128 Test error: 0.90200 Test accuracy: 0.11\n",
            "[  47]  Training error: 0.90248 Test error: 0.90260 Test accuracy: 0.1\n",
            "[  48]  Training error: 0.89782 Test error: 0.89900 Test accuracy: 0.1\n",
            "[  49]  Training error: 0.90248 Test error: 0.90260 Test accuracy: 0.1\n",
            "[  50]  Training error: 0.90085 Test error: 0.89910 Test accuracy: 0.07\n",
            "[  51]  Training error: 0.89782 Test error: 0.89900 Test accuracy: 0.1\n",
            "[  52]  Training error: 0.90248 Test error: 0.90260 Test accuracy: 0.1\n",
            "[  53]  Training error: 0.89782 Test error: 0.89900 Test accuracy: 0.1\n",
            "[  54]  Training error: 0.88763 Test error: 0.88650 Test accuracy: 0.12\n",
            "[  55]  Training error: 0.90263 Test error: 0.90180 Test accuracy: 0.12\n",
            "[  56]  Training error: 0.90128 Test error: 0.90200 Test accuracy: 0.11\n",
            "[  57]  Training error: 0.90248 Test error: 0.90260 Test accuracy: 0.1\n",
            "[  58]  Training error: 0.88763 Test error: 0.88650 Test accuracy: 0.12\n",
            "[  59]  Training error: 0.90248 Test error: 0.90260 Test accuracy: 0.1\n",
            "[  60]  Training error: 0.90248 Test error: 0.90260 Test accuracy: 0.1\n",
            "[  61]  Training error: 0.90070 Test error: 0.89680 Test accuracy: 0.09\n",
            "[  62]  Training error: 0.90128 Test error: 0.90200 Test accuracy: 0.11\n",
            "[  63]  Training error: 0.89782 Test error: 0.89900 Test accuracy: 0.1\n",
            "[  64]  Training error: 0.90128 Test error: 0.90200 Test accuracy: 0.11\n",
            "[  65]  Training error: 0.90070 Test error: 0.89680 Test accuracy: 0.09\n",
            "[  66]  Training error: 0.90128 Test error: 0.90200 Test accuracy: 0.11\n",
            "[  67]  Training error: 0.90085 Test error: 0.89910 Test accuracy: 0.07\n",
            "[  68]  Training error: 0.90137 Test error: 0.90420 Test accuracy: 0.12\n",
            "[  69]  Training error: 0.89782 Test error: 0.89900 Test accuracy: 0.1\n",
            "Done:)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# eta = 0.5\n",
        "batch_size=100;\n",
        "\n",
        "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
        "\n",
        "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
        "\n",
        "mlp.evaluate(train_data, train_labels, valid_data, valid_labels, eta=0.5,\n",
        "             eval_train=True)\n",
        "\n",
        "print(\"Done:)\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# eta = 0.005\n",
        "batch_size=100;\n",
        "\n",
        "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
        "\n",
        "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
        "\n",
        "mlp.evaluate(train_data, train_labels, valid_data, valid_labels, eta=0.005,\n",
        "             eval_train=True)\n",
        "\n",
        "print(\"Done:)\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CyxShjLWzvE",
        "outputId": "3f89060e-9037-4dd6-bfc1-db3de5c85334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating data...\n",
            "Batch size 100, the number of examples 60000.\n",
            "Batch size 100, the number of examples 10000.\n",
            "Done!\n",
            "Initializing input layer with size 784.\n",
            "Initializing hidden layer with size 100.\n",
            "Initializing hidden layer with size 100.\n",
            "Initializing output layer with size 10.\n",
            "Done!\n",
            "Training for 70 epochs...\n",
            "[   0]  Training error: 0.70337 Test error: 0.70100 Test accuracy: 0.3\n",
            "[   1]  Training error: 0.64720 Test error: 0.64300 Test accuracy: 0.4\n",
            "[   2]  Training error: 0.59818 Test error: 0.59680 Test accuracy: 0.43\n",
            "[   3]  Training error: 0.45548 Test error: 0.46660 Test accuracy: 0.56\n",
            "[   4]  Training error: 0.20697 Test error: 0.19770 Test accuracy: 0.81\n",
            "[   5]  Training error: 0.11350 Test error: 0.11090 Test accuracy: 0.88\n",
            "[   6]  Training error: 0.08995 Test error: 0.08750 Test accuracy: 0.87\n",
            "[   7]  Training error: 0.07548 Test error: 0.07410 Test accuracy: 0.89\n",
            "[   8]  Training error: 0.06405 Test error: 0.06400 Test accuracy: 0.93\n",
            "[   9]  Training error: 0.05452 Test error: 0.05620 Test accuracy: 0.94\n",
            "[  10]  Training error: 0.04775 Test error: 0.05000 Test accuracy: 0.94\n",
            "[  11]  Training error: 0.04238 Test error: 0.04510 Test accuracy: 0.94\n",
            "[  12]  Training error: 0.03763 Test error: 0.04170 Test accuracy: 0.95\n",
            "[  13]  Training error: 0.03388 Test error: 0.03940 Test accuracy: 0.95\n",
            "[  14]  Training error: 0.03075 Test error: 0.03730 Test accuracy: 0.95\n",
            "[  15]  Training error: 0.02862 Test error: 0.03470 Test accuracy: 0.95\n",
            "[  16]  Training error: 0.02688 Test error: 0.03340 Test accuracy: 0.96\n",
            "[  17]  Training error: 0.02493 Test error: 0.03270 Test accuracy: 0.96\n",
            "[  18]  Training error: 0.02302 Test error: 0.03240 Test accuracy: 0.96\n",
            "[  19]  Training error: 0.02155 Test error: 0.03170 Test accuracy: 0.96\n",
            "[  20]  Training error: 0.02037 Test error: 0.03110 Test accuracy: 0.96\n",
            "[  21]  Training error: 0.01912 Test error: 0.03010 Test accuracy: 0.96\n",
            "[  22]  Training error: 0.01807 Test error: 0.02970 Test accuracy: 0.96\n",
            "[  23]  Training error: 0.01683 Test error: 0.02960 Test accuracy: 0.96\n",
            "[  24]  Training error: 0.01592 Test error: 0.03010 Test accuracy: 0.96\n",
            "[  25]  Training error: 0.01487 Test error: 0.02950 Test accuracy: 0.96\n",
            "[  26]  Training error: 0.01407 Test error: 0.02940 Test accuracy: 0.96\n",
            "[  27]  Training error: 0.01308 Test error: 0.02940 Test accuracy: 0.97\n",
            "[  28]  Training error: 0.01233 Test error: 0.02910 Test accuracy: 0.97\n",
            "[  29]  Training error: 0.01182 Test error: 0.02920 Test accuracy: 0.97\n",
            "[  30]  Training error: 0.01133 Test error: 0.02930 Test accuracy: 0.97\n",
            "[  31]  Training error: 0.01073 Test error: 0.02930 Test accuracy: 0.97\n",
            "[  32]  Training error: 0.01013 Test error: 0.02870 Test accuracy: 0.97\n",
            "[  33]  Training error: 0.00958 Test error: 0.02890 Test accuracy: 0.96\n",
            "[  34]  Training error: 0.00902 Test error: 0.02870 Test accuracy: 0.96\n",
            "[  35]  Training error: 0.00852 Test error: 0.02890 Test accuracy: 0.96\n",
            "[  36]  Training error: 0.00812 Test error: 0.02870 Test accuracy: 0.96\n",
            "[  37]  Training error: 0.00782 Test error: 0.02860 Test accuracy: 0.96\n",
            "[  38]  Training error: 0.00737 Test error: 0.02820 Test accuracy: 0.96\n",
            "[  39]  Training error: 0.00707 Test error: 0.02830 Test accuracy: 0.96\n",
            "[  40]  Training error: 0.00677 Test error: 0.02830 Test accuracy: 0.96\n",
            "[  41]  Training error: 0.00640 Test error: 0.02840 Test accuracy: 0.96\n",
            "[  42]  Training error: 0.00617 Test error: 0.02860 Test accuracy: 0.96\n",
            "[  43]  Training error: 0.00587 Test error: 0.02910 Test accuracy: 0.96\n",
            "[  44]  Training error: 0.00568 Test error: 0.02960 Test accuracy: 0.96\n",
            "[  45]  Training error: 0.00548 Test error: 0.02960 Test accuracy: 0.96\n",
            "[  46]  Training error: 0.00503 Test error: 0.02920 Test accuracy: 0.96\n",
            "[  47]  Training error: 0.00440 Test error: 0.02870 Test accuracy: 0.96\n",
            "[  48]  Training error: 0.00360 Test error: 0.02750 Test accuracy: 0.98\n",
            "[  49]  Training error: 0.00287 Test error: 0.02720 Test accuracy: 0.98\n",
            "[  50]  Training error: 0.00247 Test error: 0.02700 Test accuracy: 0.98\n",
            "[  51]  Training error: 0.00202 Test error: 0.02690 Test accuracy: 0.98\n",
            "[  52]  Training error: 0.00178 Test error: 0.02680 Test accuracy: 0.98\n",
            "[  53]  Training error: 0.00158 Test error: 0.02670 Test accuracy: 0.98\n",
            "[  54]  Training error: 0.00155 Test error: 0.02670 Test accuracy: 0.98\n",
            "[  55]  Training error: 0.00130 Test error: 0.02690 Test accuracy: 0.98\n",
            "[  56]  Training error: 0.00130 Test error: 0.02700 Test accuracy: 0.98\n",
            "[  57]  Training error: 0.00118 Test error: 0.02700 Test accuracy: 0.98\n",
            "[  58]  Training error: 0.00108 Test error: 0.02720 Test accuracy: 0.98\n",
            "[  59]  Training error: 0.00105 Test error: 0.02710 Test accuracy: 0.98\n",
            "[  60]  Training error: 0.00098 Test error: 0.02680 Test accuracy: 0.98\n",
            "[  61]  Training error: 0.00095 Test error: 0.02660 Test accuracy: 0.98\n",
            "[  62]  Training error: 0.00082 Test error: 0.02650 Test accuracy: 0.98\n",
            "[  63]  Training error: 0.00077 Test error: 0.02640 Test accuracy: 0.98\n",
            "[  64]  Training error: 0.00062 Test error: 0.02620 Test accuracy: 0.98\n",
            "[  65]  Training error: 0.00057 Test error: 0.02630 Test accuracy: 0.98\n",
            "[  66]  Training error: 0.00053 Test error: 0.02610 Test accuracy: 0.98\n",
            "[  67]  Training error: 0.00050 Test error: 0.02640 Test accuracy: 0.98\n",
            "[  68]  Training error: 0.00045 Test error: 0.02650 Test accuracy: 0.98\n",
            "[  69]  Training error: 0.00040 Test error: 0.02640 Test accuracy: 0.98\n",
            "Done:)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of learning rate: compared to 0.05, with 0.005 the overfitting is coming in later epochs. And with 0.5 learning rate, it is clearly too big so both the train loss and test loss remains high and fluctating and not able to find the optimal point."
      ],
      "metadata": {
        "id": "dC7-H4DSqyrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1.4 Extend with Relu output"
      ],
      "metadata": {
        "id": "1M8FY4PIKqir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#functions of non-linear activations\n",
        "def f_relu(X, deriv=False, alpha=0.01):\n",
        "    if not deriv:\n",
        "      return np.maximum(X, 0)\n",
        "\n",
        "    else:\n",
        "      return np.where(X > 0, 1, X*alpha) # leaky relu\n"
      ],
      "metadata": {
        "id": "G7kZStlPLeOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPReLu(MultiLayerPerceptron):\n",
        "    def __init__(self, layer_config, batch_size=100):\n",
        "        self.layers = []\n",
        "        self.num_layers = len(layer_config)\n",
        "        self.minibatch_size = batch_size\n",
        "\n",
        "        for i in range(self.num_layers-1):\n",
        "            if i == 0:\n",
        "                print (\"Initializing input layer with size {0}.\".format(layer_config[i]))\n",
        "                # Here, we add an additional unit at the input for the bias\n",
        "                # weight.\n",
        "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
        "                                         batch_size,\n",
        "                                         is_input=True,\n",
        "                                         activation=f_relu))\n",
        "            else:\n",
        "                print (\"Initializing hidden layer with size {0}.\".format(layer_config[i]))\n",
        "                # Here we add an additional unit in the hidden layers for the\n",
        "                # bias weight.\n",
        "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
        "                                         batch_size,\n",
        "                                         activation=f_sigmoid))\n",
        "\n",
        "        print (\"Initializing output layer with size {0}.\".format(layer_config[-1]))\n",
        "        self.layers.append(Layer([layer_config[-1], None],\n",
        "                                 batch_size,\n",
        "                                 is_output=True,\n",
        "                                 activation=f_softmax))\n",
        "        print (\"Done!\")"
      ],
      "metadata": {
        "id": "qQoIN_OBLTRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# epochs = 70 and learning rate =0.05 with relu\n",
        "batch_size=100;\n",
        "\n",
        "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
        "\n",
        "mlp = MLPReLu(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
        "\n",
        "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,\n",
        "             eval_train=True)\n",
        "\n",
        "print(\"Done:)\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Gr6lrjxMm-M",
        "outputId": "72b0ef67-4ed5-4ce6-ec94-0f62d5b68ea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating data...\n",
            "Batch size 100, the number of examples 60000.\n",
            "Batch size 100, the number of examples 10000.\n",
            "Done!\n",
            "Initializing input layer with size 784.\n",
            "Initializing hidden layer with size 100.\n",
            "Initializing hidden layer with size 100.\n",
            "Initializing output layer with size 10.\n",
            "Done!\n",
            "Training for 70 epochs...\n",
            "[   0]  Training error: 0.42535 Test error: 0.42240 Test accuracy: 0.63\n",
            "[   1]  Training error: 0.09563 Test error: 0.09470 Test accuracy: 0.87\n",
            "[   2]  Training error: 0.05807 Test error: 0.06080 Test accuracy: 0.91\n",
            "[   3]  Training error: 0.04642 Test error: 0.04900 Test accuracy: 0.95\n",
            "[   4]  Training error: 0.03235 Test error: 0.04110 Test accuracy: 0.96\n",
            "[   5]  Training error: 0.03347 Test error: 0.04220 Test accuracy: 0.95\n",
            "[   6]  Training error: 0.02710 Test error: 0.03880 Test accuracy: 0.96\n",
            "[   7]  Training error: 0.02708 Test error: 0.03930 Test accuracy: 0.96\n",
            "[   8]  Training error: 0.02598 Test error: 0.04000 Test accuracy: 0.97\n",
            "[   9]  Training error: 0.02315 Test error: 0.03550 Test accuracy: 0.96\n",
            "[  10]  Training error: 0.02408 Test error: 0.03770 Test accuracy: 0.98\n",
            "[  11]  Training error: 0.01753 Test error: 0.03300 Test accuracy: 0.99\n",
            "[  12]  Training error: 0.01550 Test error: 0.03180 Test accuracy: 0.98\n",
            "[  13]  Training error: 0.01948 Test error: 0.03610 Test accuracy: 0.99\n",
            "[  14]  Training error: 0.01640 Test error: 0.03530 Test accuracy: 0.99\n",
            "[  15]  Training error: 0.01917 Test error: 0.03740 Test accuracy: 0.97\n",
            "[  16]  Training error: 0.01907 Test error: 0.03640 Test accuracy: 0.96\n",
            "[  17]  Training error: 0.01313 Test error: 0.03040 Test accuracy: 0.98\n",
            "[  18]  Training error: 0.01085 Test error: 0.02990 Test accuracy: 0.99\n",
            "[  19]  Training error: 0.01395 Test error: 0.03280 Test accuracy: 0.99\n",
            "[  20]  Training error: 0.01108 Test error: 0.03090 Test accuracy: 0.99\n",
            "[  21]  Training error: 0.01333 Test error: 0.03320 Test accuracy: 0.99\n",
            "[  22]  Training error: 0.01155 Test error: 0.03290 Test accuracy: 0.99\n",
            "[  23]  Training error: 0.01295 Test error: 0.03260 Test accuracy: 0.99\n",
            "[  24]  Training error: 0.01215 Test error: 0.03400 Test accuracy: 0.99\n",
            "[  25]  Training error: 0.01302 Test error: 0.03540 Test accuracy: 0.98\n",
            "[  26]  Training error: 0.01002 Test error: 0.03110 Test accuracy: 0.98\n",
            "[  27]  Training error: 0.01112 Test error: 0.03170 Test accuracy: 0.98\n",
            "[  28]  Training error: 0.00692 Test error: 0.02940 Test accuracy: 0.99\n",
            "[  29]  Training error: 0.00612 Test error: 0.02810 Test accuracy: 0.99\n",
            "[  30]  Training error: 0.01213 Test error: 0.03600 Test accuracy: 0.95\n",
            "[  31]  Training error: 0.00457 Test error: 0.02890 Test accuracy: 0.98\n",
            "[  32]  Training error: 0.00418 Test error: 0.02770 Test accuracy: 0.97\n",
            "[  33]  Training error: 0.00292 Test error: 0.02750 Test accuracy: 0.98\n",
            "[  34]  Training error: 0.00430 Test error: 0.02840 Test accuracy: 0.98\n",
            "[  35]  Training error: 0.00482 Test error: 0.02740 Test accuracy: 0.99\n",
            "[  36]  Training error: 0.00650 Test error: 0.03090 Test accuracy: 0.98\n",
            "[  37]  Training error: 0.00768 Test error: 0.03050 Test accuracy: 0.99\n",
            "[  38]  Training error: 0.00687 Test error: 0.03070 Test accuracy: 0.98\n",
            "[  39]  Training error: 0.01087 Test error: 0.03160 Test accuracy: 0.97\n",
            "[  40]  Training error: 0.00383 Test error: 0.02950 Test accuracy: 0.97\n",
            "[  41]  Training error: 0.00415 Test error: 0.02760 Test accuracy: 0.98\n",
            "[  42]  Training error: 0.00192 Test error: 0.02740 Test accuracy: 0.99\n",
            "[  43]  Training error: 0.00228 Test error: 0.02840 Test accuracy: 0.99\n",
            "[  44]  Training error: 0.00147 Test error: 0.02790 Test accuracy: 0.99\n",
            "[  45]  Training error: 0.00358 Test error: 0.02740 Test accuracy: 0.98\n",
            "[  46]  Training error: 0.00213 Test error: 0.02830 Test accuracy: 0.99\n",
            "[  47]  Training error: 0.00273 Test error: 0.02950 Test accuracy: 0.98\n",
            "[  48]  Training error: 0.00085 Test error: 0.02740 Test accuracy: 0.97\n",
            "[  49]  Training error: 0.00112 Test error: 0.02770 Test accuracy: 0.97\n",
            "[  50]  Training error: 0.00030 Test error: 0.02610 Test accuracy: 0.98\n",
            "[  51]  Training error: 0.00015 Test error: 0.02560 Test accuracy: 0.98\n",
            "[  52]  Training error: 0.00010 Test error: 0.02670 Test accuracy: 0.98\n",
            "[  53]  Training error: 0.00007 Test error: 0.02660 Test accuracy: 0.98\n",
            "[  54]  Training error: 0.00003 Test error: 0.02690 Test accuracy: 0.98\n",
            "[  55]  Training error: 0.00000 Test error: 0.02670 Test accuracy: 0.98\n",
            "[  56]  Training error: 0.00000 Test error: 0.02690 Test accuracy: 0.98\n",
            "[  57]  Training error: 0.00000 Test error: 0.02690 Test accuracy: 0.98\n",
            "[  58]  Training error: 0.00000 Test error: 0.02690 Test accuracy: 0.98\n",
            "[  59]  Training error: 0.00000 Test error: 0.02700 Test accuracy: 0.98\n",
            "[  60]  Training error: 0.00000 Test error: 0.02730 Test accuracy: 0.98\n",
            "[  61]  Training error: 0.00000 Test error: 0.02720 Test accuracy: 0.98\n",
            "[  62]  Training error: 0.00000 Test error: 0.02700 Test accuracy: 0.98\n",
            "[  63]  Training error: 0.00000 Test error: 0.02710 Test accuracy: 0.98\n",
            "[  64]  Training error: 0.00000 Test error: 0.02710 Test accuracy: 0.98\n",
            "[  65]  Training error: 0.00000 Test error: 0.02720 Test accuracy: 0.98\n",
            "[  66]  Training error: 0.00000 Test error: 0.02720 Test accuracy: 0.98\n",
            "[  67]  Training error: 0.00000 Test error: 0.02730 Test accuracy: 0.98\n",
            "[  68]  Training error: 0.00000 Test error: 0.02750 Test accuracy: 0.98\n",
            "[  69]  Training error: 0.00000 Test error: 0.02760 Test accuracy: 0.98\n",
            "Done:)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: I have tried learning rate range from 0.005 to 1 for relu activation(the code blocks are removed to avoid messing up), but I failed to find a proper learning rate that get the training started and not too large so the loss is fluctuating. So instead of normal relu, I actually implemented the leaky relu which can help with dying relu problem. If using the normal relu, I think implementing some weight initialize techniques can help dying relu problem."
      ],
      "metadata": {
        "id": "KzboGgHnfs0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5SPrAe8BoOX8"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}