{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 2 by Boying Li (boylil-3)"
      ],
      "metadata": {
        "id": "OHB8LzY5qR3t"
      },
      "id": "OHB8LzY5qR3t"
    },
    {
      "cell_type": "markdown",
      "id": "90928a16",
      "metadata": {
        "id": "90928a16"
      },
      "source": [
        "# Word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b3aa2d5",
      "metadata": {
        "id": "3b3aa2d5"
      },
      "source": [
        "## Dimension = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c34a9609",
      "metadata": {
        "id": "c34a9609"
      },
      "outputs": [],
      "source": [
        "# import modules & set up logging\n",
        "import gensim, logging\n",
        "import numpy as np\n",
        "import help_functions as hf\n",
        "import nltk\n",
        "\n",
        "#@author: The first version of this code is the courtesy of Vadim Selyanik\n",
        "\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "lemmatizer = nltk.WordNetLemmatizer() # create a lemmatizer\n",
        "\n",
        "sentences = []\n",
        "file = open(\"lemmatized.text\", \"r\")\n",
        "\n",
        "for line in file: # read the file and create list which contains all sentences found in the text\n",
        "    sentences.append(line.split())\n",
        "# train word2vec on the two sentences\n",
        "\n",
        "dimension = 50 # parameter for Word2vec size of vectors for word embedding\n",
        "\n",
        "# Words that occur more frequently than this threshold are randomly downsampled\n",
        "threshold = 0.00055 # parameter for Word2vec\n",
        "\n",
        "sum = 0.0\n",
        "\n",
        "# sg=1 indicates using the skip-gram model(predict context word given target word), sg=0 means using cbow\n",
        "model = gensim.models.Word2Vec(sentences, min_count=1, sample=threshold, sg=1, vector_size=dimension) # create model using Word2Ve with the given parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5dae5cc",
      "metadata": {
        "id": "d5dae5cc"
      },
      "source": [
        "Note: gensim.models.Word2Vec() train for 5 epochs default if no epochs number specified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "057de2fe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "057de2fe",
        "outputId": "41487e5b-5757-46d6-c3c8-3eb5abf46764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70583\n"
          ]
        }
      ],
      "source": [
        "# model.vocab is replaced by model.wv\n",
        "print(len(model.wv)) # check the length of the vocabulary which was formed by Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "444628da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "444628da",
        "outputId": "e242b5f3-3446-4029-a0b4-2b65e6ff0f83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baecb3f1",
      "metadata": {
        "id": "baecb3f1"
      },
      "outputs": [],
      "source": [
        "#The rest implements passing TOEFL tests\n",
        "i = 0 #counter for TOEFL tests\n",
        "number_of_tests = 80\n",
        "text_file = open('new_toefl.txt', 'r')\n",
        "right_answers = 0 # variable for correct answers\n",
        "number_skipped_tests = 0 # some tests could be skipped if there are no corresponding words in the vocabulary extracted from the training corpus\n",
        "while i < number_of_tests:\n",
        "            line = text_file.readline() #read line in the file\n",
        "            words = line.split() # extract words from the line\n",
        "            try:\n",
        "                words = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word, 'v'), 'n'), 'a') for word in\n",
        "                         words] # lemmatize words in the current test\n",
        "                vectors = []\n",
        "                if words[0] in model.wv: # check if there embedding for the query word\n",
        "                    k = 1 #counter for loop iterating over 5 words in the test\n",
        "                    vectors.append(model.wv[words[0]])\n",
        "                    while k < 5:\n",
        "                        if words[k] in model.wv: # if alternative has the embedding\n",
        "                            vectors.append(model.wv[words[k]]) #assing the learned vector\n",
        "                        else:\n",
        "                            vectors.append(np.random.randn(dimension)) #assing random vector\n",
        "                        k += 1\n",
        "                    right_answers += hf.get_answer_mod(vectors) #find the closest vector and check if it is the correct answer\n",
        "\n",
        "            except KeyError: # if there is no representation for the query vector than skip\n",
        "                number_skipped_tests += 1\n",
        "                print(\"skipped test: \" + str(i) + \"; Line: \" + str(words))\n",
        "            except IndexError:\n",
        "                print(i)\n",
        "                print(line)\n",
        "                print(words)\n",
        "                break\n",
        "            i += 1\n",
        "text_file.close()\n",
        "sum += 100 * float(right_answers) / float(number_of_tests) #get the percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a634c014",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a634c014",
        "outputId": "3616a9c7-bcb3-4ec8-fc2d-9cfddd2169df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold ferq = 0.00055 Percentage of correct answers: 70.0%\n"
          ]
        }
      ],
      "source": [
        "print(\"Threshold ferq = \"+ str(threshold)+\" Percentage of correct answers: \" + str(sum) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea72d821",
      "metadata": {
        "id": "ea72d821"
      },
      "source": [
        "## Dimension = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dc9d130",
      "metadata": {
        "id": "0dc9d130"
      },
      "outputs": [],
      "source": [
        "dimension = 5 # parameter for Word2vec size of vectors for word embedding\n",
        "\n",
        "# Words that occur more frequently than this threshold are randomly downsampled\n",
        "threshold = 0.00055 # parameter for Word2vec\n",
        "\n",
        "sum = 0.0\n",
        "\n",
        "# sg=1 indicates using the skip-gram model(predict context word given target word), sg=0 means using cbow\n",
        "model = gensim.models.Word2Vec(sentences, min_count=1, sample=threshold, sg=1, vector_size=dimension) # create model using Word2Ve with the given parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f84c4f34",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f84c4f34",
        "outputId": "65a50bca-bc89-4401-ccbd-76a16f2c5d21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70583\n"
          ]
        }
      ],
      "source": [
        "print(len(model.wv)) # check the length of the vocabulary which was formed by Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aa38f5c",
      "metadata": {
        "id": "5aa38f5c"
      },
      "source": [
        "Note: The length of model.wv is determined by the number of unique words in your training data that meet the min_count threshold, so it remains the same with different vector_size(dimension)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5219dbd8",
      "metadata": {
        "id": "5219dbd8"
      },
      "outputs": [],
      "source": [
        "#The rest implements passing TOEFL tests\n",
        "i = 0 #counter for TOEFL tests\n",
        "number_of_tests = 80\n",
        "text_file = open('new_toefl.txt', 'r')\n",
        "right_answers = 0 # variable for correct answers\n",
        "number_skipped_tests = 0 # some tests could be skipped if there are no corresponding words in the vocabulary extracted from the training corpus\n",
        "while i < number_of_tests:\n",
        "            line = text_file.readline() #read line in the file\n",
        "            words = line.split() # extract words from the line\n",
        "            try:\n",
        "                words = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word, 'v'), 'n'), 'a') for word in\n",
        "                         words] # lemmatize words in the current test\n",
        "                vectors = []\n",
        "                if words[0] in model.wv: # check if there embedding for the query word\n",
        "                    k = 1 #counter for loop iterating over 5 words in the test\n",
        "                    vectors.append(model.wv[words[0]])\n",
        "                    while k < 5:\n",
        "                        if words[k] in model.wv: # if alternative has the embedding\n",
        "                            vectors.append(model.wv[words[k]]) #assing the learned vector\n",
        "                        else:\n",
        "                            vectors.append(np.random.randn(dimension)) #assing random vector\n",
        "                        k += 1\n",
        "                    right_answers += hf.get_answer_mod(vectors) #find the closest vector and check if it is the correct answer\n",
        "\n",
        "            except KeyError: # if there is no representation for the query vector than skip\n",
        "                number_skipped_tests += 1\n",
        "                print(\"skipped test: \" + str(i) + \"; Line: \" + str(words))\n",
        "            except IndexError:\n",
        "                print(i)\n",
        "                print(line)\n",
        "                print(words)\n",
        "                break\n",
        "            i += 1\n",
        "text_file.close()\n",
        "sum += 100 * float(right_answers) / float(number_of_tests) #get the percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "842bc520",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "842bc520",
        "outputId": "d14c5f73-1804-468b-ef4f-c45e95bb5be9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold ferq = 0.00055 Percentage of correct answers: 37.5%\n"
          ]
        }
      ],
      "source": [
        "print(\"Threshold ferq = \"+ str(threshold)+\" Percentage of correct answers: \" + str(sum) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7ce4ec2",
      "metadata": {
        "id": "d7ce4ec2"
      },
      "source": [
        "## Dimension = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "946dc133",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "946dc133",
        "outputId": "2f41504f-92c3-44bc-fd75-8262222b2421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 13min 59s, sys: 2.26 s, total: 14min 1s\n",
            "Wall time: 8min 23s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "dimension = 500 # parameter for Word2vec size of vectors for word embedding\n",
        "\n",
        "# Words that occur more frequently than this threshold are randomly downsampled\n",
        "threshold = 0.00055 # parameter for Word2vec\n",
        "\n",
        "sum = 0.0\n",
        "\n",
        "# sg=1 indicates using the skip-gram model(predict context word given target word), sg=0 means using cbow\n",
        "model = gensim.models.Word2Vec(sentences, min_count=1, sample=threshold, sg=1, vector_size=dimension) # create model using Word2Ve with the given parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b117857",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b117857",
        "outputId": "11a9f648-5cc3-4a1e-f980-228206e55135"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threshold ferq = 0.00055 Percentage of correct answers: 71.25%\n"
          ]
        }
      ],
      "source": [
        "#The rest implements passing TOEFL tests\n",
        "i = 0 #counter for TOEFL tests\n",
        "number_of_tests = 80\n",
        "text_file = open('new_toefl.txt', 'r')\n",
        "right_answers = 0 # variable for correct answers\n",
        "number_skipped_tests = 0 # some tests could be skipped if there are no corresponding words in the vocabulary extracted from the training corpus\n",
        "while i < number_of_tests:\n",
        "            line = text_file.readline() #read line in the file\n",
        "            words = line.split() # extract words from the line\n",
        "            try:\n",
        "                words = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word, 'v'), 'n'), 'a') for word in\n",
        "                         words] # lemmatize words in the current test\n",
        "                vectors = []\n",
        "                if words[0] in model.wv: # check if there embedding for the query word\n",
        "                    k = 1 #counter for loop iterating over 5 words in the test\n",
        "                    vectors.append(model.wv[words[0]])\n",
        "                    while k < 5:\n",
        "                        if words[k] in model.wv: # if alternative has the embedding\n",
        "                            vectors.append(model.wv[words[k]]) #assing the learned vector\n",
        "                        else:\n",
        "                            vectors.append(np.random.randn(dimension)) #assing random vector\n",
        "                        k += 1\n",
        "                    right_answers += hf.get_answer_mod(vectors) #find the closest vector and check if it is the correct answer\n",
        "\n",
        "            except KeyError: # if there is no representation for the query vector than skip\n",
        "                number_skipped_tests += 1\n",
        "                print(\"skipped test: \" + str(i) + \"; Line: \" + str(words))\n",
        "            except IndexError:\n",
        "                print(i)\n",
        "                print(line)\n",
        "                print(words)\n",
        "                break\n",
        "            i += 1\n",
        "text_file.close()\n",
        "sum += 100 * float(right_answers) / float(number_of_tests) #get the percentage\n",
        "print(\"Threshold ferq = \"+ str(threshold)+\" Percentage of correct answers: \" + str(sum) + \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84a1af63",
      "metadata": {
        "id": "84a1af63"
      },
      "source": [
        "Summary: I have tried dimension of 5(accuracy=37.5%), 50(accuracy=70%) and 500(accuracy=71.25%). With higher dimension, the accuracy is higher because higher dimensional vectors can capture more detailed and nuanced relationships between words, but it also uses more memory and computational power and longer training time. Dimension 50 improves the accuracy significantly compared to dimension 5, while continue increase the dimension to 500 does not increase the accuracy that much. So 50 dimensional vectors are less detailed compared to 500 dimensional vectors but 50 dimensional can still capture the most important relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef71b45c",
      "metadata": {
        "id": "ef71b45c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}